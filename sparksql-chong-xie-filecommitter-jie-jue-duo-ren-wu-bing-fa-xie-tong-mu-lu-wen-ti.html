<!DOCTYPE html>
<html lang="en">

<head>
  <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<!-- iOS Safari -->
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
<!-- Chrome, Firefox OS and Opera Status Bar Color -->
<meta name="theme-color" content="#FFFFFF">
<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css">
<link rel="stylesheet" type="text/css"
  href="https://cdnjs.cloudflare.com/ajax/libs/prism/1.19.0/themes/prism.min.css">
<link rel="stylesheet" type="text/css" href="css/SourceSansPro.css">
<link rel="stylesheet" type="text/css" href="css/theme.css">
<link rel="stylesheet" type="text/css" href="css/notablog.css">
<!-- Favicon -->

  <link rel="shortcut icon" href="https://avatars1.githubusercontent.com/u/32795735?s=400&amp;u=40ed0595fb3044b01c47849f59ad96d69b4dc8db&amp;v=4">

<style>
  :root {
    font-size: 20px;
  }
</style>
  <title>SparkSql 重写 FileCommitter 解决多任务并发写同目录问题&nbsp;|&nbsp;Rianico‘s Blog</title>
  <meta property="og:type" content="blog">
  <meta property="og:title" content="SparkSql 重写 FileCommitter 解决多任务并发写同目录问题">
  
    <meta name="description" content="关于多个 SparkSql 作业写同个目录的问题">
    <meta property="og:description" content="关于多个 SparkSql 作业写同个目录的问题">
  
  
    <meta property="og:image" content="https://raw.githubusercontent.com/Rianico/Image/master/Avator/sparkles%20(1).png">
  
  <style>
    .DateTagBar {
      margin-top: 1.0rem;
    }
  </style>
</head>

<body>
  <nav class="Navbar">
  <a href="index.html">
    <div class="Navbar__Btn">
      
        <span><img class="inline-img-icon" src="https://avatars1.githubusercontent.com/u/32795735?s=400&amp;u=40ed0595fb3044b01c47849f59ad96d69b4dc8db&amp;v=4"></span>&nbsp;
      
      <span>Home</span>
    </div>
  </a>
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
  
    
      <span class="Navbar__Delim">&centerdot;</span>
      <a href="about.html">
        <div class="Navbar__Btn">
          
            <span><img class="inline-img-icon" src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2F88bfa795-fb50-42ad-b5f9-821f35ee821d%2Fgithub_(2).png?table=block&amp;id=457ca390-7a74-4a2f-9c26-1613b0d49d9c"></span>&nbsp;
          
          <span>About</span>
        </div>
      </a>
    
  
</nav>
  <header class="Header">
    
      <div class="Header__Cover">
        <img src="https://www.notion.so/signed/https%3A%2F%2Fs3-us-west-2.amazonaws.com%2Fsecure.notion-static.com%2Fc5f33405-3269-47ac-96a2-fbcb8271d259%2Fphoto-1509910110001-4e756f86fbd3.jfif?table=block&amp;id=37fe08e9-84c8-4029-aec4-db298a672aeb">
      </div>
    
    <div class="Header__Spacer ">
    </div>
    
      <div class="Header__Icon">
        <span><img class="inline-img-icon" src="https://raw.githubusercontent.com/Rianico/Image/master/Avator/sparkles%20(1).png"></span>
      </div>
    
    <h1 class="Header__Title">SparkSql 重写 FileCommitter 解决多任务并发写同目录问题</h1>
    
      <div class="DateTagBar">
        
          <span class="DateTagBar__Item DateTagBar__Date">Posted on Mon, Nov 29, 2021</span>
        
        
          <span class="DateTagBar__Item DateTagBar__Tag DateTagBar__Tag--default">
            <a href="tag/Spark.html">Spark</a>
          </span>
        
      </div>
    
  </header>
  <article id="https://www.notion.so/37fe08e984c84029aec4db298a672aeb" class="PageRoot PageRoot--FullWidth"><ul id="https://www.notion.so/ec7c8aeb567143c5a502eb2a624ee836" class="ColorfulBlock ColorfulBlock--ColorGray TableOfContents"><li class="TableOfContents__Item"><a href="#https://www.notion.so/5c4498ae07a24cee848d44678bb79b21"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SparkSql 并发写同目录问题</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/5b654d9508df4803b7b8919f540becc6"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">关于对象存储</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/fd8421801a334a9c84f0956d32c420f4"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Spark 写 HDFS 的一致性思考</strong></span></span></div></a></li><li class="TableOfContents__Item"><a href="#https://www.notion.so/a2c24c6f3cab45009819e7eec4637f24"><div style="margin-left:0px"><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">参考</strong></span></span></div></a></li></ul><h3 id="https://www.notion.so/5c4498ae07a24cee848d44678bb79b21" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/5c4498ae07a24cee848d44678bb79b21"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">SparkSql 并发写同目录问题</strong></span></span></h3><div id="https://www.notion.so/09a181843c124164b57f70f94287498d" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">近期在将 RDD 转 Dataframe 写入 HDFS 时，发现当有多个 Spark 作业同时写入数据到同一目录下，就会产生 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString"> 目录的并发安全问题。</span></span></p></div><div id="https://www.notion.so/93f87fce052a44179097f6749121ea13" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">Spark 写入数据到 HDFS 时，为了保证数据的一致性，会先将数据写入 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString"> 目录，再执行 commitTask -&gt; commitJob 进行两次 Rename 操作，将 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString">  目录下的文件全部输出到最终目标下，再删除  </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString">  目录：</span></span></p></div><div id="https://www.notion.so/6a698d2702274d72a8ca5a393bc500ae" class="Image Image--Normal"><figure><a href="https://gitee.com/zhxuankun/Image/raw/master/blog/image-20211125091648818.png?width=816"><img src="https://gitee.com/zhxuankun/Image/raw/master/blog/image-20211125091648818.png?width=816" style="width:816px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/9fc9e680f9f2467081aec46f4ac8643a" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在这种机制下，当多个作业同时写相同目录，一旦某个作业先完成，其他作业在 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString"> 下的数据就会被被错误地删除。</span></span></p></div><div id="https://www.notion.so/bfb83a38ecac452ea46461df661db323" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在网上搜索一番后，并没有发现 SparkSql 修改 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString"> 的方案，基本都是 RDD 调用 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">saveAsHadoopFile</code></span><span class="SemanticString"> API 保存数据时，直接传递一个 conf 进去，但 SparkSql 并没有提供类似的 API，因此无法按照相同方式解决。</span></span></p></div><div id="https://www.notion.so/32cec4b7b324408989fcaaddbab178df" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">查看 SQL 执行计划，发现 SparkSql 使用了 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">InsertIntoHadoopFsRelationCommand</code></span><span class="SemanticString"> 执行数据写入操作，其中有个 committer 的变量构造如下：</span></span></p></div><pre id="https://www.notion.so/53db0d005cb64063859b7e4aac665c86" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span> val committer = FileCommitProtocol.instantiate(
       sparkSession.sessionState.conf.fileCommitProtocolClass,
       jobId = java.util.UUID.randomUUID().toString,
       outputPath = outputPath.toString,
       dynamicPartitionOverwrite = dynamicPartitionOverwrite)</span></span></span></code></pre><div id="https://www.notion.so/805f7c1212b34b8d9e1cd5cd347a6ba1" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">方法中会通过 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">sparkSession.sessionState.conf.fileCommitProtocolClass</code></span><span class="SemanticString"> 决定使用哪个类构造 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">FileCommitProtocol</code></span><span class="SemanticString">：</span></span></p></div><pre id="https://www.notion.so/4dd40caad1d94327a28d9dad00f7bc96" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span> val FILE_COMMIT_PROTOCOL_CLASS =
     buildConf(&quot;spark.sql.sources.commitProtocolClass&quot;)
       .version(&quot;2.1.1&quot;)
       .internal()
       .stringConf
       .createWithDefault(
         &quot;org.apache.spark.sql.execution.datasources.SQLHadoopMapReduceCommitProtocol&quot;)</span></span></span></code></pre><div id="https://www.notion.so/7a3378276bc1484cb6b2591fa9b84655" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">跟踪 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">SQLHadoopMapReduceCommitProtocol</code></span><span class="SemanticString"> 代码，可以看到内部还会构造一个 committer，这个 committer 才是真正决定数据写入路径的类：</span></span></p></div><pre id="https://www.notion.so/9d529a16296a450889e016607206c850" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span>     val configuration = context.getConfiguration
     val clazz =
       configuration.getClass(SQLConf.OUTPUT_COMMITTER_CLASS.key, null, classOf[OutputCommitter])
 
     if (clazz != null) {
       logInfo(s&quot;Using user defined output committer class ${clazz.getCanonicalName}&quot;)
       if (classOf[FileOutputCommitter].isAssignableFrom(clazz)) {
         val ctor = clazz.getDeclaredConstructor(classOf[Path], classOf[TaskAttemptContext])
         committer = ctor.newInstance(new Path(path), context)
       } else {
         val ctor = clazz.getDeclaredConstructor()
         committer = ctor.newInstance()
       }
     }</span></span></span></code></pre><div id="https://www.notion.so/c3377452d9844ed49e1397ef77b3b929" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">可以看到，</span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">committer 是通过反射进行构造的，并且只有当 committer 为 </strong></span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">FileOutputCommitter</strong></code></span><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold"> 的子类时，才会将上下文信息传递进去，否则使用一个无参构造器构造 committer</strong></span><span class="SemanticString">。</span></span></p></div><div id="https://www.notion.so/8b40c1d48b764b9a82dd09e0360346fc" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">由于我们需要修改 committer 的路径行为，因此无参构造器的方式先否决掉。因此只能考虑通过  </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">context</code></span><span class="SemanticString"> 传递自定义配置，重写 FileOutputCommitter 处理传进去的自定义配置，修改各个作业的 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">_temporary</code></span><span class="SemanticString"> 路径。</span></span></p></div><div id="https://www.notion.so/fb80aef90be14640a77585cbee6ba1f3" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">其中 context 是基于 SparkConf 构建的 Hadoop 上下文，Spark 默认会把 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">spark.hadoop.xxx</code></span><span class="SemanticString"> 的配置参数，注入到 Hadoop 的上下文中。因此在构造 SparkConf 的时候，就需要将自定义的外部配置传递进去：</span></span></p></div><pre id="https://www.notion.so/d57cce30367d4d60a95e4a2ec02b6f17" class="Code"><code><span class="SemanticStringArray"><span class="SemanticString"><span> val conf = new SparkConf()
     // 设置自定义的 OutputCommitter
     .set(&quot;spark.sql.sources.outputCommitterClass&quot;， classOf[SelfFileOutputCommitterV2].getName)
     // 设置为 _xxx_temporary 路径
     .set(SelfFileOutputCommitterV2.CUSTOM_PENDING_DIR_NAME, &quot;xxx&quot;)
     // 设置写入成功的标志文件名为 _xxx_SUCCESS
     .set(SelfFileOutputCommitterV2.CUSTOM_SUCCEEDED_FILE_NAME , &quot;xxx&quot;)
     // 设置不清理临时目录，作为测试验证使用
     .set(SelfFileOutputCommitterV2.FILEOUTPUTCOMMITTER_CLEANUP_SKIPPED, &quot;true&quot;)</span></span></span></code></pre><blockquote id="https://www.notion.so/5f86525b0a4242938445ed175cafb6ac" class="ColorfulBlock ColorfulBlock--ColorDefault Quote"><span class="SemanticStringArray"><span class="SemanticString">NOTE：一旦 SparkConf 在各个 executor 构造为 SparkContext 后，无法再进行修改，因此需要在最开始就传递进去。</span></span></blockquote><div id="https://www.notion.so/1a2c24e91179430faeb483ecb2b3d420" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">自定义 Committer 详见 </span><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://gitee.com/zhxuankun/code_repo/blob/master/hadoop/org/apache/hadoop/mapreduce/lib/output/SelfFileOutputCommitterV2.java">hadoop/org/apache/hadoop/mapreduce/lib/output/SelfFileOutputCommitterV2.java · zxk/code_repo - 码云 - 开源中国 (gitee.com)</a></span><span class="SemanticString"> 。</span></span></p></div><h3 id="https://www.notion.so/5b654d9508df4803b7b8919f540becc6" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/5b654d9508df4803b7b8919f540becc6"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">关于对象存储</strong></span></span></h3><div id="https://www.notion.so/2348428c7bef4d05b452c0d5b0129d31" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">同时还了解到，当 Spark 写 Amazon S3 对象存储时，传统的 Hadoop OutputCommitter 的 Rename 机制已经不适用了。Reame 操作对于 S3 来说是一个很昂贵的操作，需要拆解为 List、Copy、Delete 等多个操作。</span></span></p></div><div id="https://www.notion.so/09c058c41a00451ca7ebe6f68283c045" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">S3 为上传的文件提供了最终一致性，首先为该次上传申请一个 Upload id，接着将文件切割一块块上传，期间文件对外部不可见。待到全部上传完成后，再向 S3 发送一个完成信号，S3 会将文件进行合并，之后才对外可见；或者向 S3 发送一个失败信号，S3 删除文件。</span></span></p></div><div id="https://www.notion.so/a4dcf2b968ee4192955975b578872927" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">因此出现了基于 S3 Multipart Upload 机制的 Committer（Amazon S3 提供），流程如下：</span></span></p></div><div id="https://www.notion.so/89731d93bcf24cefbcee415a09b7cc05" class="Image Image--Normal"><figure><a href="https://gitee.com/zhxuankun/Image/raw/master/blog/image-20211125143325421.png?width=768"><img src="https://gitee.com/zhxuankun/Image/raw/master/blog/image-20211125143325421.png?width=768" style="width:768px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><h3 id="https://www.notion.so/fd8421801a334a9c84f0956d32c420f4" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/fd8421801a334a9c84f0956d32c420f4"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">Spark 写 HDFS 的一致性思考</strong></span></span></h3><div id="https://www.notion.so/3036135ed48c4b41830b02e1f2aa7322" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">再次回顾下 Spark 写入文件的两个阶段：</span></span></p></div><div id="https://www.notion.so/df8e661b4ad647dc87177089a5d0fce5" class="Image Image--Normal"><figure><a href="https://gitee.com/zhxuankun/Image/raw/master/blog/image-20211125091648818.png?width=816"><img src="https://gitee.com/zhxuankun/Image/raw/master/blog/image-20211125091648818.png?width=816" style="width:816px"/></a><figcaption><span class="SemanticStringArray"></span></figcaption></figure></div><div id="https://www.notion.so/c2ee8ac56cf74a708ed4ac0942f3cae6" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">虽然网上各种资料都说是为了保证数据的一致性，避免出现重复数据，但在 commitJob 阶段，这个动作仍是一个个文件 rename 到目标目录下的，这个操作并不是原子性的。</span></span></p></div><div id="https://www.notion.so/302fd59b256b4519b1919a9bf0a375e5" class="ColorfulBlock ColorfulBlock--ColorDefault Text"><p class="Text__Content"><span class="SemanticStringArray"><span class="SemanticString">在源码中追溯到 Spark 的 </span><span class="SemanticString"><code class="SemanticString__Fragment SemanticString__Fragment--Code">taskAttempt</code></span><span class="SemanticString"> 主要是与当前时间戳相关的。当写入方式为 Append 时，作业失败后重算，此时时间戳已发生变化，作业并无法识别到之前的数据，导致重复的数据写入。</span></span></p></div><h3 id="https://www.notion.so/a2c24c6f3cab45009819e7eec4637f24" class="ColorfulBlock ColorfulBlock--ColorDefault Heading Heading--3"><a class="Anchor" href="#https://www.notion.so/a2c24c6f3cab45009819e7eec4637f24"><svg width="16" height="16" viewBox="0 0 16 16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a><span class="SemanticStringArray"><span class="SemanticString"><strong class="SemanticString__Fragment SemanticString__Fragment--Bold">参考</strong></span></span></h3><ul class="BulletedListWrapper"><li id="https://www.notion.so/bd236273fa574ce584628bd115a63bb6" class="BulletedList"><span class="SemanticStringArray"><span class="SemanticString"><a class="SemanticString__Fragment SemanticString__Fragment--Link" href="https://bruce.blog.csdn.net/article/details/87955023">聊一聊Spark写文件的机制——如何保证数据一致性_茅庐-CSDN博客</a></span></span></li></ul></article>
  <footer class="Footer">
  <div>&copy; Rianico‘s Blog 2019-2021</div>
  <div>&centerdot;</div>
  <div>Powered by <a href="https://github.com/dragonman225/notablog" target="_blank"
      rel="noopener noreferrer">Notablog</a>.
  </div>
</footer>
</body>

</html>