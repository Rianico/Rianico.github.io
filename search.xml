<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[ARTS_Tip]]></title>
    <url>%2F2019%2F09%2F07%2FARTS-Tip%2F</url>
    <content type="text"><![CDATA[0x01Scala隐式转换使用隐式转换对类的功能进行拓展，通常可以用于扩展第三方类的方法（如添加show()展示元素），而无需派生（extends、with等）出新的类型。以拓展String类型为例，将String作为入参（其他类型同理），通常使用隐式扩展功能，要么返回一个已定义的class（spark源码使用的方式），要么隐式定义一个新的class，如下： 12345678910111213141516171819202122232425262728293031323334353637383940object Solution1 &#123;class RichString(str: String) &#123; def read() = &#123; println(str + "_fun") &#125; def some() = &#123; if(StringUtils.isBlank(str)) None else Some(str) &#125; &#125; object Context &#123; implicit val imStr: String = "this is an implict" // 返回一个已定义的class implicit def fun(str: String) = new RichString(str) // 新定义一个class implicit class myString(str: String) &#123; def read2() = &#123; println(str + "_class") &#125; &#125; &#125; def main(args: Array[String]): Unit = &#123; import Context._ "".read() "".read2() println("a".some.getClass.getTypeName) println("".some.getClass.getTypeName) &#125; &#125;// 输出：_fun_classscala.Somescala.None$ 隐式转换函数在同个作用域下，与名字无关，与入参类型有关。 0x02Docker内配置git账户最近几天在学习docker的时候，使用Dockerfile构建image，但在配置ssh时需要拷贝一份可以访问git的ssh-key，然后注入image，并且想要在Dockerfile直接运行git clone等操作也会提示失败，需要对Dockerfile作出比较复杂的配置，详见：Using SSH keys inside docker container。 因此最后决定使用https方式，通过在github端生成tokens的url，接着配置在Dockerfile里的RUN即可，详见Fetching private GitHub repos from a Docker container。 github端： Your Profile → Settings → Personal Access Tokens → Generate New Token生成token，然后配置 1git config --global url."https://&#123;token&#125;:@github.com/".insteadOf "https://github.com/" 其实无论ssh-key方式还是https方式，都会导致image中存储这些访问信息，生产环境下不推荐使用。 0x03偏函数（PartialFunction）、部分应用函数（Partial Applied Function）和柯里化(Currying)偏函数在scala中，可以利用模式匹配来扩展创建偏函数，偏函数是一种特殊的一元函数，并不支持处理所输入参数的所有情况。类型为PartialFunction[-T,+V]。T是接收的类型，V是返回的结果类型 如下，下列偏函数是求平方根，并且在input为负数的时候不做处理。 123val squareRoot: PartialFunction[Double, Double] = &#123; case x if x &gt;= 0 =&gt; Math.sqrt(x)&#125; 可以使用isDefinedAt来确认该偏函数是否能够处理某个特定的值。 12squareRoot.isDefinedAt(2) shouldEqual truesquareRoot.isDefinedAt(-2) shouldEqual false 偏函数之间彼此也可以使用orElse 或者 andThen相互连接。 123456789101112131415val positive: PartialFunction[Int, Int] = &#123; case x if x &gt;= 0 =&gt; x&#125;val odd: PartialFunction[Int, Boolean] = &#123; case x if x % 2 == 1 =&gt; true&#125;val even: PartialFunction[Int, Boolean] = &#123; case x if x % 2 == 0 =&gt; true&#125;val evenCheck: PartialFunction[Int, Boolean] = positive andThen evenval oddCheck: PartialFunction[Int, Boolean] = positive andThen odd compose可以进行一次预处理。 123456789101112// 只处理1val parfun1:PartialFunction[Int,Int] = &#123;case 1 =&gt; 1&#125; parfun1(1) // 1parfun1.isDefinedAt(2) // false// 做一层预处理val compfun:Function1[String,Int] = parfun1.compose&#123; case x=&gt;System.out.println("compose:" + x);Integer.valueOf(x)&#125;compfun(1)// compose:1// 1 偏函数的这项特性，在实现一个验证系统的时候非常有用，我们可以实现一系列的检查去验证输入的数据是否符合要求。 1val finalCheck = check1 andThen check2 andThen check3 ... 当新增或者移除的时候也十分方便。 Scala也可以将偏函数应用于collections，collect方法将一个偏函数作为输入参数，将这个偏函数作用于各个值上，跳过超出函数中定义的元素。 12345val greaterThan20: PartialFunction[Any, Int] = &#123; case i: Int if i &gt; 20 =&gt; i&#125;List(1, 45, 10, "blah", true, 25) collect greaterThan20 shouldEqual List(45, 25) 部分应用函数（Partially Applied Functions）在函数式编程中，对一个有参数的函数的调用也可以看成将函数应用到参数上。 当一个函数带着所有参数被调用时，就叫做将函数完整（fully applied）的应用到每个参数上。当只传递参数的子集到函数也是可以通过的，将返回一个部分应用函数（Partially Applied Functions）。Scala中不会因为你只传递部分参数而抛出一个异常，而是简单的将其应用并返回一个新的带着剩余需要你传递的参数的函数。 12345val divide = (num: Double, den: Double) =&gt; &#123; num / den&#125;val halfOf: (Double) =&gt; Double = divide(_, 2)halfOf 20 shouldEqual 10 我们在求某个值的一般的时候，可以知道分母必定为2，所以halfOf通过部分应用除的函数得到求半的功能。_占位符表示剩余的参数。 部分应用函数很容易跟Scala中的柯里化（Currying）混淆。两者都能减少函数中的参数数量，但是柯里化将部分应用函数的概念进行了更加深层次的扩展。 柯里化是将一个带有多个参数的函数拆分为一系列函数，每个函数带有单独的参数。 1val curriedDivide: (Double) =&gt; (Double) =&gt; Double curriedDivide函数的类型为(Double) =&gt; (Double) =&gt; (Double)，表示将divid函数拆分为两个函数，每个函数各自带着一个参数，参数根据原本的顺讯决定。 123val halfOf: (Double) =&gt; Double = curriedDivide(_)(2)halfOf(20) shouldEqual 10 柯里化跟部分应用函数的好处是能够根据原本的通用函数，创建出一个特定的函数而不用创建新的代码，使得代码较为简洁，避免冗余。 0x04Scala中的Left与RightScala中的Either代表两个可能的值中的其中一个，有Left跟Right组成。除了作为二选一之外，通常也可作为异常值的处理，约定Left代表异常值，Right代表正常输出（与Try等价，也是代表异常的一种）。 Left，Right可以用match匹配（跟Option类似）： 12345678910111213val in = Console.readLine("Type Either a string or an Int: ")val result: Either[String,Int] = try &#123; Right(in.toInt) &#125; catch &#123; case e: Exception =&gt; Left(in)&#125;println( result match &#123; case Right(x) =&gt; "You passed me the Int: " + x + ", which I will increment. " + x + " + 1 = " + (x+1) case Left(x) =&gt; "You passed me the String: " + x&#125;) Either还提供了投影（projection）操作，可单独对任意一边进行操作，也可改变类型，但是最终结果仍为原本有值的那边。 123val l: Either[String, Int] = Left("flower")l.left.map(_.size): Either[Int, Int] // Left(6)l.right.map(_.toDouble): Either[String, Double] // Left("flower") fold：对两边同时进行操作。 swap：对两边的内容进行交换。 merge：合并两边的结果，通常为Any。 与Try[A &lt;: Exception, B] 可相互进行转化： 123456789101112import scala.util.&#123; Either, Failure, Left, Right, Success, Try &#125;implicit def eitherToTry[A &lt;: Exception, B](either: Either[A, B]): Try[B] = &#123; either match &#123; case Right(obj) =&gt; Success(obj) case Left(err) =&gt; Failure(err) &#125;&#125;implicit def tryToEither[A](obj: Try[A]): Either[Throwable, A] = &#123; obj match &#123; case Success(something) =&gt; Right(something) case Failure(err) =&gt; Left(err) &#125; 0x05马克飞象与mathpix给大家推荐两个提高工作效率的软件 马克飞象：一个很方便的markdown编辑器，支持与印象笔记同步绑定，主要是使用简单，没有那么多繁琐的界面之类的， mathpix：数学公式利器，只要通过快捷键截取数学公式，即可自动转为markdown语法。 0x06关于mkfs.ex4的inode最近在工作中遇到服务器磁盘格式化的事后，对相关查询命令进行了一点挖掘，命令如下： 1mkfs.ext4 /dev/sda1 查询资料后发现，上述命令存在着优化空间，可以增大bytes per inode来节省部分磁盘空间，默认按照每16k建立一个inode来得到inode数量。inode：用于存储文件的元数据，inode越多，磁盘可以存放的文件数量越多，但是过多的inode会占用磁盘空间（一个inode占用128/256字节），若磁盘上的文件都是大文件，显然16k会导致建立较多不必要的inode，从而浪费磁盘空间。 123456## -T 指定默认参数，其中largefile 每1M建立一个inode## -n 为模拟运行，但不格式化mkfs.ext4 -T largefile /dev/sdc1## 查看每个inode占用的字节dumpe2fs -h /dev/sda1 | grep "Inode size" 访问文件的实际流程：访问(ls -i)文件（夹）名对应的inode号码-&gt;通过inode号码，获取inode信息-&gt;根据inode信息，找到文件数据所在的block，读取数据。 所以文件（夹）实际上只存储了名字跟inode号码，如果只有读权限，那么只能访问到名字，其余信息都存在inode里，而访问inode则需要执行权限（X）。 0x07Coursera最近发现有个国外的网络课程平台Coursera，有各种知名大学教授的课程，其中台湾大学林轩田教授的机器学习基石课程对于没什么相关基础的人来说，是一个很好的入门课程，在此推荐下。 0x08StringBuilder的一些使用事项关于StringBuilder，有几个需要注意的使用特性： 非线程安全(重要)，使用到StringBuffer的场景很少，因为几个线程轮流append的场景以目前的工作经验来看并没遇到，而且方法间并不保障线程安全。 123456789101112object PathUtil &#123; val buffer = new StringBuffer() def buildPath(time: String): String = &#123; // 多线程调用buildPath时，append之间的操作也不是线程安全的 buffer.append(time) buffer.append(time) buffer.toString() &#125; &#125; Java中一次性使用多个+拼接字符串时，会自动优化为一个StringBUilder()拼接多个字符串，分为多条语句使用+则会有多个StringBilder(可查看编译后的字节码)。 设定好StringBuilder(int length)的初始长度，可以避免发生内部char[]数组发生扩容（类似ArrayList扩容）。eg：假如使用StringBuilder的默认长度，会为129长度的字符串拼接，合共申请625字符的数组。 重用StringBuilder()，可以使用clear()方法，进行重用，StringBuilder的toString()源码为return new String(value, 0, count)，clear()方法本质上是将count置为0。重用的时候要注意线程安全，使用ThreadLocal，但要权衡使用的代价。此处可以参考BigDecimal源码的写法： 1234567891011121314public class BigDecimal extends Number implements Comparable&lt;BigDecimal&gt; &#123; // ...... private static final ThreadLocal&lt;StringBuilderHelper&gt; threadLocalStringBuilderHelper = new ThreadLocal&lt;StringBuilderHelper&gt;() &#123; @Override protected StringBuilderHelper initialValue() &#123; return new StringBuilderHelper(); &#125; &#125;; // ......&#125; 参考： StringBuilder在高性能场景下的正确用法 springside/springside4 0x09Regex tutorial — A quick cheatsheet by examplesA quick cheatsheet by examples 该文章介绍了几个最常用的正则表达式，且条理清晰，层次分明，此处做个总结学习。 锚——^和$ regex remark ^The 匹配以The开头的字符 end$ 匹配以end结尾的字符 ^The end$ 匹配The end的字符串 roar 匹配任何包含roar的文本 量词—— * + ? 和{} regex remark abc* 匹配ab后面跟着0个或者多个c的字符 abc? 匹配ab后面跟着0个或者1个c的字符 abc+ 匹配ab后面跟着1个或者多个c的字符 abc{2} 匹配ab后面跟着2个c的字符 abc{2,} 匹配ab后面跟着2个或者多个c的字符 abc{2,5} 匹配ab后面跟着2到5个c的字符 a(bc)* 匹配a后面跟着0个或者多个bc的字符 a(bc){2,5} 匹配a后面跟着2到5个bc的字符 或操作—— | 或 [] regex remark a(b|c) 匹配a后面跟着b或c的字符 a[bc] 同上 字符类—— \d \w \s 和 . regex remark \d 匹配一个数字的字符 \w 匹配一个单词的字符（字母数字下划线） \s 匹配一个空白的字符（包括tabs跟换行符） . 匹配任意字符 .需要谨慎使用，因为类或否定字符类(我们将在下一节讨论)通常更快更精确。 对于\d, \w 和 \s，其大写通常表示否定，比如： 1\D 匹配一个非数字的字符 有时候需要匹配一些特殊字符，如 ^.[$()|*+?{\，此时需要在前面添加\进行转义: 1\$\d 匹配前面为$的数字字符 一些无法直接打印出来的字符也可以匹配，如tab \t，换行符 \n及\r。 标记正则表达式经常使用/划分为两块，前面部分为上述提到的匹配符，后半部分则为标记： regex remark g(glabal) 第一次匹配后不停止，继续匹配，最后返回全部匹配到的字符 m(multi-line) 若指定了^和$，则会匹配多行后并返回 i(insensitive) 忽略大小写区别 分组，捕获 regex remark a(bc) 括号内创建了一个具有bc值的分组 a(?:bc)* 使用?:禁用分组 a(?&lt; foo&gt;bc) 使用?&lt;foo&gt;将分组命名为foo 假设有下列字符 1abc ac acb aob a2b a42c A87d 使用a(bc)*匹配，部分匹配结果： 使用a(?:bc)*匹配，部分匹配结果： 分组特性很有用，当我们需要从匹配到的字符中提取信息的时候，分组会以数组的形式存在在Full match中，我们只需要指定下标即可访问对应分组。 中括号——[] regex remark [abc] 匹配a或b或c的字符 [a-c] 同上 [0-9] 匹配0-9的数字字符 [^a-zA-Z] 匹配非a-z，A-Z的字符，^表示反义 在中括号[]中，所有特殊符号（如^，$）都会失去特殊意义变成普通的字符，而不需要使用\进行转义。 贪心与懒匹配量词* + {}属于贪心操作，它们会尽可能远的匹配字符。 例如，使用&lt;.+&gt;去匹配&lt;div&gt;simple div&lt;/div&gt; 会得到&lt;div&gt; simple div&lt;/div&gt;。若要只匹配&lt;div&gt;，则可以使用&lt;.+?&gt; 让其进行懒匹配。 更好的解决方案是不使用.而是使用更加严谨的表达式&lt;[^&lt;&gt;]+&gt; 边界符——\b和\B regex remark \babc\b 匹配单独一个单词abc，需要与其他字符隔开（如空格），类似于锚^和$，但不强制局限于每行的开头与结尾 \Babc\B 匹配abc，不需要与其他单词隔开，类似于锚^和$，但不强制局限于每行的开头与结尾 0x0AKafka越界查询命令Kafka查看消费者是否越界： 首先列出topic对应consumer group的当前offset及其最新可消费的offset。 1kafka-run-class kafka.tools.ConsumerOffsetChecker --zookeeper zkhost:port --topic topic_name --group topic_name_group 再列出topic的最早offset。 1kafka-run-class kafka.tools.GetOffsetShell --topic topic_name --time -2 --broker-list kafkahost:port 使用excel对partition排序，就能确认消费者消费的offset是向前还是向后越界。 python数组遍历python遍历数组且带下标： 12345678_list = ['a', 'b', 'c', 'd', 'e']## 使用enumeratefor idx, i in enumerate(_list): print('index：%s，value：%s' % (idx, i))print('====================================')## 使用range(start, end, step)for idx in range(0, len(_list), 1): print('index：%s，value：%s' % (idx, _list[idx])) 0x0B使用fsck校验hdfs文件完整性及recoverLease修复文件hdfs文件系统可以使用fsck来check(校验)文件完整性，语法格式：hdfs fsck [file] [options] 以下option，由上往下需要相继结合使用： options comment -files 校验时展示文件信息 -blocks 展示文件的块信息，通常与-files结合使用 -locations 展示文件块所在的datanode地址 -racks 展示文件机架信息 12345## 检查某文件是否完整hdfs fsck /zxk/sample.jar -file -blocks -locations -racks## 检查某文件夹下文件是否完整hdfs fsck /zxk 示例输出： 12345678910111213141516171819202122232425Connecting to namenode via http://localhost:50070/fsck?ugi=hdfs&amp;files=1&amp;blocks=1&amp;racks=1&amp;path=%2Fdata%2Fsw%2Fhttp%2FCZ%2F20190805%2F0000%2Fpart-0FSCK started by hdfs (auth:SIMPLE) from /node1for path /zxk/sample.jar at Fri Aug 23 10:47:39 CST 2019/zxk/sample.jar 185606618 bytes, 1 block(s): OK0. BP-1492896228-node2-1553135500323:blk_3942942813_2869317147 len=185606618 Live_repl=2 [/c/S9312-2/node3:50010, /c/S9312-1/node4:50010]Status: HEALTHY Total size: 185606618 B Total dirs: 0 Total files: 1 Total symlinks: 0 Total blocks (validated): 1 (avg. block size 185606618 B) Minimally replicated blocks: 1 (100.0 %) Over-replicated blocks: 0 (0.0 %) Under-replicated blocks: 0 (0.0 %) Mis-replicated blocks: 0 (0.0 %) Default replication factor: 2 Average block replication: 2.0 Corrupt blocks: 0 Missing replicas: 0 (0.0 %) Number of data-nodes: 104 Number of racks: 2FSCK ended at Fri Aug 23 10:47:39 CST 2019 in 1 millisecondsThe filesystem under path &apos;/zxk/sample.jar&apos; is HEALTHY 假如文件有异常的话，我们可以观察到两种信息： Under replicatedCORRUPT blockpool 修复Under replicated：这种情况意味着还有其他replication存活，可使用recoverLease命令进行恢复： 12## 寻找under replicated的文件hdfs fsck / | grep -i "under replicated" | awk '&#123;print $1&#125;' |sort | uniq | sed -e 's/://g'&gt; under_replicated.flst 接下来使用recoverLeae命令修复： 123## [-path path] HDFS path for which to recover the lease.## [-retries num-retries] Number of times the client will retry calling recoverLease. The default number of retries is 1.for f in `cat under_replicated.flst` &#123; echo "Fixing $f" ; hdfs debug recoverLease -path $f; &#125; 修复corrupt blocks： 12## 查找出损坏的文件并删除 hdfs fsck / | egrep -v '^\.+$' | grep -v eplica| awk -F ':' '&#123;system("sudo -u hdfs hadoop fs -rm -r "$1" 2&gt;&amp;1")&#125;' 参考：How to fix corrupt HDFS FIlesHow to use hdfs fsck command to identify corrupted files? 0x0CDocker部署Elastic可以通过docker快速部署一个elastic search，步骤如下： 12345## 拉取elastic官方镜像docker pull docker.elastic.co/elasticsearch/elasticsearch:7.3.1## 创建container，并指定端口为9200,9300docker run -p 9200:9200 -p 9300:9300 -e "discovery.type=single-node" docker.elastic.co/elasticsearch/elasticsearch:7.3.1 对于elastic的其他版本，可以到Docker @ Elastic下载. 参考链接：https://www.elastic.co/guide/en/elasticsearch/reference/current/docker.html 0x0DLinux下排查磁盘文件占满 首先查看真实文件占用 123## -s, --summarize 只分别计算当前路径下每个文件(夹)的总占用## -x, --one-file-system 忽略处于不同文件系统上的目录du -shx /xxx 查看对应目录下逻辑上的磁盘占用情况 1df -h /xxx 查看被进程占用的文件，并kill对应的pid 12## sort：-n 作为number排序， -k1 以第一列作为key排序，-t 可以指定分隔符lsof -s|grep deleted|sort -n -k1 如果df与du仍然不一致，根据具体场景，umount掉不必要的目录后，再次使用du查看占用，之后rm -rf占用大的文件夹内容，再重新挂载。 word文件中插入代码可以将代码黏贴到该网站，然后格式化后复制黏贴到word中：http://pygments.org/ Linxu下free命令输出相关123456## 使用free查看free -h total used free shared buffers cachedMem: 251G 243G 8.6G 3.8M 3.7G 120G-/+ buffers/cache: 119G 132G Swap: 124G 24M 124G 从操作系统(Mem)出发： Mem.total = Mem.used + Mem.free used：包含了buffers以及cached buffers：表示被系统buffer的内存，主要用于存放要输出到disk的数据。 cached：表示被系统cahce的内存，主要用于存放从disk读取的数据。 buffer跟cache都是为了提高系统I/O性能。 从程序角度(-/+ buffers/cache)出发： used：表示应用程序认为已经使用了多少内存，不包含buffers以及cached，且(-/+buffer/cache).used=Mem.used-Mem.buffer-Mem.cache free：表示应用程序认为系统还有多少内存，由于buffers以及cached都可以被系统快速回收，因此算在free里。(-/+buffer/cache).free = Mem.free + Mem.buffers + Mem.cached 大致布局如下：]]></content>
      <categories>
        <category>ARTS</category>
      </categories>
      <tags>
        <tag>scala</tag>
        <tag>Docker</tag>
        <tag>python</tag>
        <tag>Linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ARTS_Review]]></title>
    <url>%2F2019%2F09%2F06%2FARTS-Review%2F</url>
    <content type="text"><![CDATA[0x01_9 tips about using cats in Scala you might want to know9 tips about using cats in Scala you might want to know 此篇主要是介绍了scala的9个tips，主要是一些实际工程中较好的实践，但是本人目前英文水平尚浅，scala又较难，只能看到一些比较表面的东西，值得时不时的回顾。scala是一门基于jvm生态的十分灵活的语言，但也正因为太灵活了，因此使用难度也较高，但其中的一些思想及用法，很值得深思及细细品味。 0x02_Goodbye, Object Oriented ProgrammingGoodbye, Object Oriented Programming 这篇文章主要是对面向对象编程里的继承，封装跟多态里存在的一些问题进行了讨论。 针对继承（Inheritance）： 由于继承的层次过深，往往需要从一个项目拷贝一个东西到另一个项目的时候，需要拷贝更多的对象（如父类，父类的父类等），形象的比喻为：只想要一根香蕉，却给了一只拿着香蕉的猩猩跟整片森林。 经典的菱形继承问题，只能使用多态解决 脆弱的基类问题， 基类： 123456789101112131415161718import java.util.ArrayList; public class Array&#123; private ArrayList&lt;Object&gt; a = new ArrayList&lt;Object&gt;(); public void add(Object element) &#123; a.add(element); &#125; public void addAll(Object elements[]) &#123; for (int i = 0; i &lt; elements.length; ++i) // 调用了本地array的add方法 a.add(elements[i]); // this line is going to be changed &#125;&#125; 派生类，新增了count进行计数： 12345678910111213141516171819public class ArrayCount extends Array&#123; private int count = 0; @Override public void add(Object element) &#123; super.add(element); ++count; &#125; @Override public void addAll(Object elements[]) &#123; // 调用父类的addAll()方法 super.addAll(elements); count += elements.length; &#125;&#125; 上述代码原本能正常工作，但是有一天基类ArrayList发生了改变： 123456public void addAll(Object elements[]) &#123; for (int i = 0; i &lt; elements.length; ++i) // 注意： 由于派生类重写了add方法，因此add方法此时执行的是派生类的方法 add(elements[i]); // this line was changed &#125; 在上述改变中，ArrayList的修改是合情合理的，并且在原本的所有代码也能通过测试，但对于派生类来说，它原本期待的行为发生了改变。 原本ArrayCount addAll()调用基类的addAll()，期待由基类的addAll()方法完成操作，但是基类内部却调用了由子类重写的add()方法，从而导致重复计数！ And they must be informed about every change in the Base class since it could break their Derived class in unpredictable ways. 从上述情况来看，子类的作者必须知道父类是如何实现的。而且他们必须被告知父类的每个变化，因为它们可能以一种不可预知的方式破坏子类。 通过上述例子，可能内心会对继承这个特性开始感到怀疑。但更加本质的是，怀疑通过继承进行层次分类的方案的可行性。 现实世界实际上有很层次结构，比如一双袜子在你的抽屉里在你的梳妆台在你的房间在你的房子等。所以其实层次分类是好的，只不过当前面向对象编程使用的模型不够好。 作者提出了使用tag来标记类从而进行分类（其实就跟scala的特质思想很类似了）。 比如职工手册，就有这几个tag：Document，Company，Handbook 针对封装（Encapsulation） 虽然表面看起来，封装隐藏了对象内部状态，保证了对象内部状态的安全。但是，由于OOP中处于性能的考虑，传递给一个方法的通常是一个对象的引用，那么此时假如将一个Object传递给一个构造函数，再传递给另一个构造函数，那么两个构造函数同时拥有了对这个Object的引用，从而导致该Object不是安全的。 解决方法：对Object进行clone，而且必须是deep clone，而且Object中的Object也要进行deep clone。。。（其实可以通过腌咸菜的方式来避免多层的deep clone） 多态（Polymorphism）对于多态，作者认为多态只是起到一个支援作用，完全可以通过Interface-base来混进多种行为，并且也没有限制。 作者最后表示摒弃OOP，转向函数式编程。 0x03_How to think like a programmer — lessons in problem solvingHow to think like a programmer — lessons in problem solving 作者认为解决问题的能力是最重要的，并且也是雇主最看重的，胜过编码熟练度，调试，系统设计等。 Have a framework解决问题好的方式分两步：a）有一套框架b）重复执行直到解决问题 Understand一个问题之所以难是因为无法确实的搞明白，如何才能将其搞明白？能够以逻辑的方式将其明确的叙述一遍。当你将其从头梳理叙述一边的时候，可能就可以立刻看到之前一些没注意到的逻辑上的漏洞。可以将其画在纸上，或者小黄鸭调试法等。 “If you can’t explain something in simple terms, you don’t understand it.” — Richard Feynman Plan做每件事之前，不要贸然的直接做，而要进行一次规划，比如可以这样问自己：假设输入了X，那么要输出Y的必要步骤是什么？ Divide（important）学会将问题拆分，不要试图解决一个大的问题，而是将其拆分为多个小问题，一个一个解决，最后再连接到一起。 Stuck即便按照上述步骤走，有时候仍然是会卡住，这个是每个人都会遇到的，只不过那些最杰出的程序员比起焦虑对错误的地方持有更大的兴趣。 0x04_Strings are not the type you are looking forStrings are not the type you are looking for 这篇文章开头抛砖引玉的谈论了下静态类型语言以及动态类型语言，然后强调了静态类型语言的编译时类型检查的优点。接着以 java.lang.String class举例，提出了滥用String的缺点： 多个类型全都是String的话欠缺类型精度，失去了type的意义。 各个参数容易混淆，比如调用一个方法查询电影的ID，需要传入一个actor id实例，结果传入了actor name，类型检查却能通过。 为了避免以上情况，scala可以通过value class（Kotlin则是data class）对基本类型进行包装，很容易的实现合适的自定义type，通过继承Anyval实现，形如： 12345678910111213141516// note that the 'Person.' prefix can be omittedcase class Person(name: Person.Name, age: Person.Age, id: Person.Id)object Person &#123; case class Name(value: String) extends AnyVal &#123; override def toString = value.toString &#125; case class Age(value: Int) extends AnyVal &#123; override def toString = value.toString &#125; case class Id(value: String) extends AnyVal &#123; override def toString = value.toString &#125;&#125; 通过使用伴生类的内部类来描述相关类型，同时避免类名冲突。 性能考虑： In summary, Value classes are a mechanism in Scala to avoid allocating runtime objects. 值类在scala中的机制会避免在运行时分配对象。 值类只能继承一些通用的trait比如AnyVal，或者继承了Any的trait，而无法对自身进行扩展。通用的trait定义的方法可以对值类进行扩展，但要使用的话，会导致其实例化： 1234567trait Printable extends Any &#123; def print(): Unit = println(this)&#125;class Wrapper(val underlying: Int) extends AnyVal with Printableval w = new Wrapper(3)w.print() // actually requires instantiating a Wrapper instance 0x05_Choosing a Reactive Framework for the JVMChoosing a Reactive Framework for the JVM 文中主要是由一次项目经历，提到了响应式框架，由此围绕如何选择响应式框架讨论，按照时间线，介绍了其出现的原因，各个框架的出现以及主要解决什么问题，只有需要处理的请求规模到达一定程度（如让服务器同时处理超过1万个请求），响应式编程才能起到一个至关重要的作用。稳重提到的几个响应式框架都是围绕Scale，Latency，Resilience，Event Driven Domains来这几个主题设计的，但是使用响应式框架的门槛比较高，而且不一定能带来收益。 0x06_Machine Learning is Fun! Part 2Machine Learning is Fun! Part 2 这是一篇系列博文，隐藏了很多机器学习中的细节，以一种简洁的方式去描述机器学习，旨在让那些对机器学习也有兴趣的但毫无经验的人也能够看明白，part2主要讲的是神经网络，列举了几个例子来描述了神经网络一些有趣的应用。 0x07_Differences Between AI and Machine Learning, and Why it MattersDifferences Between AI and Machine Learning, and Why it Matters 最近，出炉了一份关于企业在他们的产品或服务滥用AI的报告。根据这份报告，约40%的欧洲创业公司宣称使用了AI但实际并没有使用到了。去年，也有一些使用了AI技术的公司在他们的产品中滥用AI技术获取了数以千计用户的数据。 即使在当今，仍然有许多群众跟媒体仍然对AI跟机器学习混淆不清，两者的概念经常被当做同一个，此外，也被许多企业利用该噱头用于大肆宣传，以提高他们的收益，而忽略在某些方面上两者属于一个平行的领域。 什么是机器学习？ 机器学习是AI的一个分支，Tom M. Mitchell对其作出以下定义： Machine learning is the study of computer algorithms that allow computer programs to automatically improve through experience. 机器学习是研究能够通过经验自动提升计算机程序表现的算法。 具体例子有音乐推荐系统，比较成功的应用例子有Netflix, Spotify,以及国内一些厂商（比如网易云，QQ音乐，etc）。 这里简单介绍几种机器学习类型及其简单的定义： 有监督学习：通过找到之前输入的数据（nput features）及目标输出（target prediction output）之间的关联及依赖，从而能够根据新的输入预测输出。 无监督学习：主要是用于模式捕获及描述建模。这类算法没有数据输出的标签。（这类模型通常用未打上标签的数据进行训练） 强化学习：这类学习主要目标是通过与环境交互获得的观察结果从而让收益最大化或者尽可能的减小风险。从这点出发的话，强化学习算法会持续不断迭代的从环境中学习。一个很好的例子就是dota2训练电脑知道能够与人类对战。 机器学习是十分具有吸引力的，尤其是其中的两个子分支（如深度学习和神经网络）。事实上，尽管有些人在尝试比较深度学习和神经网络与人类大脑工作方式的区别，两者之间仍有很大不同。 什么是AI？ AI，从另一方面来说，是一个非常广泛的领域。按照Carnegie Mellon University的话： Artificial intelligence is the science and engineering of making computers behave in ways that, until recently, we thought required human intelligence. 从近代来看，AI是能够让计算机能够表现出跟人类相似智能的科学与工程。但这也仅能提现这个领域是多么的广泛。在五十年前，一个下棋游戏程序就能够被称为AI，因为博弈是只有人类大脑才有的功能，但放在今天来看，一个下棋游戏是相当地之能的，几乎在每个OS都能找到这游戏。因此“until recently”是随着时间发展而进化的。 我们今天所知道的AI，主要代表有谷歌Home、Siri和Alexa等人工智能互动设备，以及Netflix、亚马逊(Amazon)和YouTube等以机器学习为动力的视频预测系统。这些现金技术在我们日常生活中逐渐变得重要。事实上，他们主要是作为智能助手帮助我们提高生产力。 对比机器学习，AI是一个不断发展的概念，它的定义也会随机技术的发展而做出改变。可能在几十年后，在今天属于十分创新的AI技术到那时也会被当做是很落后的技术。 0x08_How a simple mix of object-oriented programming can sharpen your deep learning prototypeHow a simple mix of object-oriented programming can sharpen your deep learning prototype文章主要是面向那些没有软件开发背景的数据科学家和机器学习从业者，呼吁在代码中可以适当的引入OOP原则，编写注释文档等，提高代码的可读性跟可用性。文章里的内容相对于软件开发背景的人来说，都是一些较日常的原则，可以当做锻炼英语来看看。 0x09_Why Kubernetes Is Awesome: A Beginner’s Guide.Why Kubernetes Is Awesome: A Beginner’s Guide. 这篇文章对于k8s做了一个初步的介绍。 k8s是一个软件，允许我们部署，管理，扩展我们的的应用。应用会被打包在容器中，k8s将他们分组成单元。k8s允许我们将数以千计的服务内联起来，而从外部看来它们就只是一个单独的单元。 假设有个用户发起了请求，该请求通过负载均衡重定向到了k8s集群里的一个节点，该节点处理完请求后，返回响应。 Note：一个节点是在集群内的一台机器（虚拟/物理）。每个节点可以包含多个pod，一个pod可以包含一个或者多个container。pod的概念对于多个容器很有用，如果它们需要向文件系统一样分享资源的话。 k8s的使用主要由以下五个步骤组成： 开发一个应用创建一个简单的Node.js应用——一个简单的HTTP server，可以返回OS的hostname及平台信息。 123456789101112const os = require('os');const http = require('http');const requestHandler = (req, res) =&gt; &#123; console.log('Request incoming from ' + req.connection.remoteAddress); res.writeHead(200); res.end('Success! You\'ve hit ' + os.hostname() + ' on ' + os.platform() + '!');&#125;;const server = http.createServer(requestHandler)server.listen(3000); 使用node index.js查看效果。请确保本地环境的node又在浏览器的localhost:3000运行。 容器化应用编辑Dockerfile文件 123FROM node:ltsADD index.js /index.jsCMD node index.js 确保目录下只有Dockerfile跟前面编写的index.js文件，tag定为simple-http，执行下列命令： 1docker build -t simple-http . 运行该image： 1docker run --rm --name k8s -p 3000:3000 -d imgId 确保没问题后，将其上传到自己的Dockerhub： 123docker tag simple-http &lt;&lt;your_docker_username&gt;&gt;/simple-httpdocker push &lt;&lt;your_docker_username&gt;&gt;/simple-http 创建一个k8s集群登录GCP: Google Cloud Platform. ，安装Cloud SDK，使用该工具安装kubectl： 1gcloud components install kubectl 创建一个新项目： 1gcloud projects create simple-http-project — set-as-default 创建一个congtainer集群： 1gcloud container clusters create simple-http-kubes --num-nodes 2 --machine-type f1-micro --region us-east1 由于谷歌账户的限制，最多只能创建2个ndoe。上述步骤之后，可以看到如下信息： 12AME LOCATION MASTER_VERSION MASTER_IP MACHINE_TYPE NODE_VERSION NUM_NODES STATUSsimple-http-kubes us-east1 1.10.9-gke.5 35.229.48.250 f1-micro 1.10.9-gke.5 6 RUNNING 将container部署到群1kubectl run simple-http-kubes --image=&lt;&lt;your_docker_username&gt;&gt;/simple-http --port=3000 --generator=run/v1 暴露并扩展集群通过负载均衡，暴露集群 1kubectl expose rc simple-http-kubes --type=LoadBalancer --name simple-http 等待直到取得负载均衡对应的IP，可以使用下列命令查看： 1kubectl get services 现在有多个k8s节点，但只有一个副本，使用下列命令进行集群扩展： 1kubectl scale rc simple-http-kubes --replicas=10 最后等到一切都执行完成（使用kubectl get pods验证），如果你多次使用curl命令进行查询，应该可以得到多个不同的hostname 1Success! You&apos;ve hit simple-http-kubes-s6l7w on linux! 0x0A_Machine learning &amp; Kafka KSQL stream processing — bug me when I’ve left the heater onMachine learning &amp; Kafka KSQL stream processing — bug me when I’ve left the heater on 这篇文章主要是讲述如何使用机器学习+KSQL(KAFKA SQL)实时监控家庭电力的使用，以便在电力使用异常的时候进行告警。 通过收集几个月家里用电情况的数据后，通过使用H2O.ai（一个开源的Spark机器学习框架），训练出一个模型，再编写KSQL的UDF（需要一些java基础，方法打包成jar包），将模型嵌进去，从而能够持续的监控电力使用情况。 做完以上步骤后，在假期偶尔会有误告警（在作者出门度假的时候），因此仍需要更多的假期数据进行训练。 0x0B_Here’s why so many data scientists are leaving their jobHere’s why so many data scientists are leaving their jobs 文章作者是数据科学家从业者，文章开头提到 Stack Overflow的一份报告，报告表明，14.3%的数据科学家再寻找新工作，位居第一，第二位则是机器学习工程师。由此展开讨论在当前数据科学家这份工作的一些缺点以及优点。 期望与现实总是差距很大，并非所有但绝大多数的公司在连基础的设备条件下招聘年轻的数据科学家，再加上这类公司往往也没有这方面相关的资深人才，这两者会结合起来没法使人开心起来。数据工程师想要写出足够智能的机器学习算法从而进行挖掘，然而通常他们需要做的第一件事情是搭建环境以及每天出分析报告。相反的，公司只是需要每天在会议上能够出示一份表格。并且公司不久也会觉得他们的投入并没有获得足够的汇报，这些都会导致数据科学家这个角色的人不开心。另一个原因就是数据科学家总是想着做一些能够对人们产生影响的事情，但是如果公司的核心业务不是寄去学习，数据科学家做的好像就只能提供一些收入的增长。 权力争斗，只有好的技术是不够的，还需要领导者对你有一个好的印象。这意味着你得经常做一些额外的特殊工作，比如在正确的时间从数据库中取得一些数字给正确的人看，做一些简单无谓的项目以让正确的人对你有个好的印象。即使这很无聊，但很有必要。 跟随上述第二点，必须取悦的领导往往不知道“数据科学家”代表什么。这意味着你最后只是时分析专家，报告专家以及数据库专家。不仅非技术出身的领导层会错误的定位你的技能，你的同事也往往认为你知道任何数据相关的东西，比如Spark，Hadoop，Hive，Pig，SQL，Neo4J，Python等。（如果有哪家公司的招聘基本全写了，那么要注意，可能他们自身都知道他们的数据战略是什么，他们可以雇佣数据相关的人可以解决他们所有的问题。） 0x0C_IssuesAbout Kafka1. Automated leader rebalance causes replication downtime for clusters with too many partitionsAutomated leader rebalance causes replication downtime for clusters with too many partitions 12345678Type: BugStatus: RESOLVEDPriority: MajorResolution: FixedAffects Version/s: 0.8.2.2, 0.9.0.0, 0.9.0.1, 0.10.0.0, 0.10.0.1 Fix Version/s: 1.1.0Component/s: controllerLabels: reliability Kafka集群在开启auto.leader.rebalance.enable后（默认开启），如果集群中有较多数量的partition，那么在重启的时候将会有大量的replication处于不可服务的时间。这回导致UnderReplicatedPartitions大量爆发，并且副本停止。 本质上是因为leader重选举平衡的时候，对于不平衡的partition立刻改变leader，而没有进行恰当的处理。这个操作实际上停止了集群中所有的replica fetcher。在繁忙的集群中，这个可能会持续数分钟，在这期间副本复制也不会发生，从而会导致用户数据处于不可靠的状态。 实际上在0.11版本也不会见到上述问题，应该是已经修复了。 2. batch LeaderAndIsr requests during auto preferred leader electionbatch LeaderAndIsr requests during auto preferred leader election 12345678Type: Sub-taskStatus:RESOLVEDPriority: MajorResolution: FixedAffects Version/s: 2.0.0Fix Version/s: 1.1.2, 2.0.1, 2.1.0Component/s: coreLabels: None 当前，Controller发送LeaderAndIsrRequest的时候是一个partition发送一次。为了提高效率，可以将多个paritition的LeaderAndIsrRequest打包为一个batch再发送。 About Spark1. Spark performance tuning from the trenchesSpark performance tuning from the trenches 1.借助Tungsten的力量，Tungsten是Spark做出的一个有史以来最大的改动，为了能够最大限度的使用CPU及能存，为了充分使用Tungsten，有以下几点需要注意： 使用Dataset而不是DataFrames，使用DataSet有很多好处，但也需要权衡DataSet在map跟filter的function上性能也较差。 尽可能不要使用UDFs以及UDAFs，而是尽量使用一些内建（built-in）的方法。这样才能被Spark充分的优化，参考例子：UDF’s vs Spark sql vs column expressions performance optimization]]></content>
      <categories>
        <category>ARTS</category>
      </categories>
      <tags>
        <tag>article</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[ARTS_Algorithm]]></title>
    <url>%2F2019%2F09%2F06%2FARTS-Algorithm%2F</url>
    <content type="text"><![CDATA[0x01- 两数之和 两数之和 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那 两个 整数，并返回他们的数组下标。 你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例: 给定 nums = [2, 7, 11, 15], target = 9 因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] 解法一 暴力循环法，直接写两层循环即可，此处不作示例。 解法二 为了减少时间复杂度，采用空间换时间，即使用Hash对每个对象做一个类似索引的功能，同时加上不能重复利用数组中同一个元素限制，因此在对nums进行循环的时候立即对map进行查找，即可很自然的绕过该限制,时间复杂度为O(1)。 12345678910111213def twoSum(nums: Array[Int], target: Int): Array[Int] = &#123; import scala.collection.mutable.Map val map:Map[Int,Int] = Map() for (x &lt;- nums.indices) &#123; val comp = target - nums(x) if (map.contains(comp)) &#123; return Array(map.get(comp).get, x) &#125; else &#123; map += (nums(x)-&gt; x) &#125; &#125; Array(0, 0)&#125; 时间上以及空间上均可取得较为不错的性能。 解法三 $$其实就是常规的两层循环，只不过内循环开始的下标为外层循环+1，减少了循环的次数，时间复杂度仍为O(n^2)$$ 12345678910111213import scala.collection.mutableobject Solution &#123; def twoSum(nums: Array[Int], target: Int): Array[Int] =&#123; val size = nums.size nums.indices.foreach &#123; idx1 =&gt; val comp = target - nums(idx1) for (idx2 &lt;- idx1 + 1 until size) &#123; if(comp == nums(idx2)) return Array(idx1,idx2) &#125; &#125; Array() &#125;&#125; 内存消耗较少，但在数组长度较长的时候性能不如做Hash索引好。 0x02- Linked List Cycle II142. Linked List Cycle II Given a linked list, return the node where the cycle begins. If there is no cycle, return null. To represent a cycle in the given linked list, we use an integer pos which represents the position (0-indexed) in the linked list where tail connects to. If pos is -1, then there is no cycle in the linked list. Note: Do not modify the linked list. Example 1: Input: head = [3,2,0,-4], pos = 1Output: tail connects to node index 1Explanation: There is a cycle in the linked list, where tail connects to the second node. Example 2: Input: head = [1,2], pos = 0Output: tail connects to node index 0Explanation: There is a cycle in the linked list, where tail connects to the first node. Example 3: Input: head = [1], pos = -1Output: no cycleExplanation: There is no cycle in the linked list. Follow-up:Can you solve it without using extra space? 根据结尾最后的提示，不要使用额外的空间，也就是可以直接在数组内进行循环求得，此处可以假设有两个不同速度的A和B，采用步长的方法，错开，然后直到相遇后就可以求得。 假设A速度为1，B速度为2，x个循环后相遇，那么两者走过的长度分比为x，2x。假设循环开始的点称为cycle_node，从起点start_node-&gt;cycle_node的距离为start_cycle。假设A,B相遇的点为meeting_node。在两者相遇的那个点，可以得到两个信息： A走过的路程为x，cycle_node-&gt;meeting_node的距离为x - start_cycle。 B多走的路程为2x - x，即meeting_node走一圈回到自身的距离为x，其中有段距离为cycle_node-&gt;meeting_node的距离，用x减去这段距离：（x - （x - start_cycle））= start_cycle，即可得知meeting_node-&gt;cycle_node = start_cycle 综上可得出，只要求出meeting_node的位置，然后开始循环，同时从start_node也开始循环，两者相遇时即为cycle_node。 12345678910111213141516171819202122232425262728293031323334353637/** * Definition for singly-linked list. * class ListNode &#123; * int val; * ListNode next; * ListNode(int x) &#123; * val = x; * next = null; * &#125; * &#125; */public class Solution &#123; public ListNode detectCycle(ListNode head) &#123; if(null == head) return null; ListNode fast = head; ListNode slow = head; // get meeting_node while(null != fast &amp;&amp; null != fast.next)&#123; fast = fast.next.next; slow = slow.next; if( fast == slow) break; &#125; if(null == fast || null == fast.next) return null; while(head != fast)&#123; head = head.next; fast = fast.next; &#125; return head; &#125;&#125; 0x03- 3Sum15. 3Sum 从Example中输出的结果是有序来看： 先将数组进行排序 外部循环数组，将外部循环当前的元素作为target值并取负值 内部对当前下标后面的数组再进行首尾循环，两者都向中间靠拢，若两者相加小于target，则head向后位移，反之同理。 为避免输出重复结果，每次添加结果后都需要对head，tail与变化前的值作对比，相同则跳过。 外部循环数组的元素也同理，需要做对比并跳过相同元素。 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647def threeSum(nums: Array[Int]): List[List[Int]] = &#123; if (nums.size &lt; 3) List.empty[List[Int]] import scala.collection.mutable.ArrayBuffer // prepare the return result val res = ArrayBuffer.empty[List[Int]] // sort val _nums = nums.sorted var idx = 0 while(idx &lt; nums.size - 2) &#123; // get the target per loop val target = -_nums(idx) // head and tail of the Array var head = idx + 1 var tail = nums.length - 1 // while (head &lt; tail) &#123; if (_nums(head) + _nums(tail) == target) &#123; res += List(_nums(idx), _nums(head), _nums(tail)) // head++ while to avoid the same args while(head &lt; tail &amp;&amp; _nums(head + 1) == _nums(head))&#123; head = head + 1 &#125; // tail-- while to avoid the same args while(tail &gt; head &amp;&amp; _nums(tail - 1) == _nums(tail))&#123; tail = tail - 1 &#125; head = head + 1 tail = tail - 1 &#125; else if (_nums(head) + _nums(tail) &lt; target) &#123; head = head + 1 &#125; else &#123; tail = tail - 1 &#125; &#125; idx = idx + 1 // idx++ to avoid the same args while(idx &lt; nums.size &amp;&amp; _nums(idx) == _nums(idx -1))&#123; idx = idx + 1 &#125; &#125; res.toList &#125; 0x04- Single Number136. Single Number 思路：从一个数组中，每个元素都会出现两次，只有其中一个会出现一次，找出只出现一次的那个元素。由异或^可知，相同值之间异或为0，且A^B^C=A^C^B，0与任何值做异或操作，不会改变其值，因此只要对数组聚合做异或操作，即可得到只出现一次的值。 12345object Solution &#123; def singleNumber(nums: Array[Int]): Int = &#123; nums.reduce((x, y) =&gt; x ^ y) &#125;&#125; 0x05- 单词搜索79. 单词搜索 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162636465666768697071727374package rianico.my.leetcode/*@see https://leetcode-cn.com/problems/word-search/给定一个二维网格和一个单词，找出该单词是否存在于网格中。单词必须按照字母顺序，通过相邻的单元格内的字母构成，其中“相邻”单元格是那些水平相邻或垂直相邻的单元格。同一个单元格内的字母不允许被重复使用。示例:board =[ ['A','B','C','E'], ['S','F','C','S'], ['A','D','E','E']]给定 word = "ABCCED", 返回 true.给定 word = "SEE", 返回 true.给定 word = "ABCB", 返回 false.思路：算法主要考察的是深度优先搜索（DFS），深度优先搜索算法通常结合递归来进行。1. 明确为DFS后，通常考虑到递归；2. 由于题目限定不可以使用重复的元素，因此每访问一个元素需要打个标记代表已访问；3. 若某条路线失败，需要回溯上去，此时要把之前打过的标记清除；4. 设置好各种界限条件，注意路线不能超出数组范围*/import scala.collection.mutableobject Solution79 &#123; def exist(board: Array[Array[Char]], word: String): Boolean = &#123; val _board = board.map(_.toBuffer) // 使用Hash防止数组较大时访问速度慢 val map = mutable.HashMap.empty[(Int, Int), Boolean] for (xidx &lt;- board.indices) &#123; for (yidx &lt;- board(xidx).indices) &#123; if (find(xidx, yidx, 0, _board, word)) return true &#125; &#125; false &#125; def find(xidx: Int, yidx: Int, word_idx: Int, board: Array[mutable.Buffer[Char]], word: String): Boolean = &#123; if (word_idx &gt;= word.length) &#123; return true &#125; else if (xidx &lt; 0 || xidx &gt;= board.length || yidx &lt; 0 || yidx &gt;= board(xidx).length) &#123; return false &#125; else if (board(xidx)(yidx) != word.charAt(word_idx)) &#123; return false &#125; val tmp = board(xidx)(yidx) board(xidx)(yidx) = '*' if (find(xidx + 1, yidx, word_idx + 1, board, word) | find(xidx, yidx + 1, word_idx + 1, board, word) | find(xidx - 1, yidx, word_idx + 1, board, word) | find(xidx, yidx - 1, word_idx + 1, board, word)) &#123; return true &#125; board(xidx)(yidx) = tmp false &#125; def main(args: Array[String]): Unit = &#123; println(exist(Array(Array('a', 'a', 'a', 'a'), Array('a', 'a', 'a', 'a'), Array('a', 'a', 'a', 'a')), "aaaaaaaaaaaa")) // Expect： false &#125;&#125; 0x06- Word Ladder127. Word Ladder 12345678910111213141516171819202122232425262728293031323334'''每次都只替换一个字母，求beginWord到endWord的最少变换次数，由最少一般可以联想到广度优先搜索，从而将问题转化为一个广度优先的问题。为了优化效率，采用双头广度优先搜索的方法，及从顶层开始求下一层，同时目标点也往上走，直到两头到达同一层，即可得出最优解。广度优先搜索核心思想：每次都求出下一层的所有节点，并打上标记，不断深入即可。'''class Solution: def ladderLength(self, beginWord: str, endWord: str, wordList: List[str]) -&gt; int: word_set = set(wordList) if endWord not in word_set: return 0 letters = 'qwertyuiopasdfghjklzxcvbnm' length = len(beginWord) begin_set, end_set = &#123;beginWord&#125;, &#123;endWord&#125; count = 1 while(begin_set): count += 1 tmp_set = set() for word in begin_set: for idx in range(length): head, tail = word[:idx], word[idx+1:] for l in letters: tmp_word = head + l + tail if tmp_word in end_set: return count if tmp_word in word_set: ## 访问过的节点打上标记，此处为去除该参数 word_set.remove(tmp_word) tmp_set.add(tmp_word) ## 双头靠拢 if len(tmp_set) &gt; len(end_set): begin_set, end_set = end_set, tmp_set else: begin_set, end_set = tmp_set, end_set return 0 0x07- Flipping an ImageFlipping an Image 1234## 很简单的数组循环，倒序，再取反或者用1^进行异或操作都可以class Solution: def flipAndInvertImage(self, A: List[List[int]]) -&gt; List[List[int]]: return list(map(lambda x: list(reversed(list(map(lambda y: 1^y,x)))), A)) 0x08- Sort Array By Parity905. Sort Array By Parity 123456## 此处使用lambda会比自己用数组拼接快很多class Solution: def sortArrayByParity(self, A: List[int]) -&gt; List[int]: A.sort(key = lambda x : x % 2) return A ## return ([x for x in A if x % 2 == 0] + [x for x in A if x % 2 == 1]) 0x09- Squares of a Sorted Array977. Squares of a Sorted Array 12345## 对其求平方后排序class Solution: def sortedSquares(self, A: List[int]) -&gt; List[int]: ## return sorted(list(map(lambda x: pow(x, 2), A))) return sorted([pow(a,2) for a in A]) 561. Array Partition I 12345678910## 升序排序后，将复数下标相加即可class Solution: def arrayPairSum(self, nums: List[int]) -&gt; int: res = 0 nums.sort() for i in range(0, len(nums), 2): res = res + nums[i] return res ## list(...)[start_idx:end_idx:step] ## return sum(sorted(nums)[::2]) 0x0A- Height Checker1051. Height Checker 123456789## 先对数组进行排序后，对比两者不同的元素并计数。class Solution1051: def heightChecker(self, heights: List[int]) -&gt; int: count = 0 for i, j in zip(heights, sorted(heights)): if(i != j): count += 1 return count ## return len([0 for i, j in zip(heights, sorted(heights)) if i!=j ]) - Sort Array By Parity II922. Sort Array By Parity II 12345678910## 使用步长2循环，奇数位1开始判断是否错位class Solution: def sortArrayByParityII(self, A: List[int]) -&gt; List[int]: j = 1 for i in range(0, len(A), 2): if A[i] % 2: while A[j] % 2: j = j + 2 A[i], A[j] = A[j], A[i] return A 0x0B- Find Words That Can Be Formed by Characters1160. Find Words That Can Be Formed by Characters 1234567891011121314151617181920212223242526272829303132333435## 对所有字母做一个索引放到list里，元素为chars各个字母出现的次数，接着做双重循环，遇到一次则-1class Solution1160: def countCharacters(self, words: List[str], chars: str) -&gt; int: arr = [0] * 26 for i in chars: arr[ord(i)-97] = arr[ord(i)-97] + 1 length = 0 for word in words: flag = True _arr = arr.copy() for character in word: idx = ord(character) - 97 _arr[idx] = _arr[idx] - 1 flag = flag &amp; (_arr[idx] &gt;= 0) if not flag: break if flag: length += len(word) return length## leetcode最佳解法，使用str.count()计字符串中出现的次数,若word小于chars出现的次数，则逮代表通过class Solution: def countCharacters(self, words: List[str], chars: str) -&gt; int: ret = 0 for word in words: boolean = True for i in word: if word.count(i) &gt; chars.count(i): boolean = False break if boolean: ret += len(word) return ret 0x0C- Fibonacci Number509. Fibonacci Number 123456789## 经典的求裴波那契数列的问题，此处需要注意若使用常规的递归，而不做缓存，会导致每个裴波那契数都会对前面的子裴波那契数进行重复计算，可以使用缓存，避免重复计算，从而降低时间复杂度class Solution: def fib(self, N: int) -&gt; int: list = [0, 1] for i in range(2, N + 1): list.append(list[i - 1] + list[i - 2]) return list[N] ## 使用黄金比例与斐波那契数列的关系 ## return int((((1 + 5 ** 0.5) / 2) ** N + 1) / 5 ** 0.5)]]></content>
      <categories>
        <category>ARTS</category>
      </categories>
      <tags>
        <tag>Algorithm</tag>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石-- Lecture4_Learning is Impossible?]]></title>
    <url>%2F2019%2F08%2F18%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3--Lecture4_Learning-is-Impossible%2F</url>
    <content type="text"><![CDATA[Learning is Impossible?考虑一个例子，根据黑白格子的分布，来确定一个二分类问题，判断$g(x)$为+1还是-1。 由于一种现象可能存在多种解释，所以往往没有一个确定的答案，这时，我们可以说在机器学习中，无论使用了怎样的算法，机器都无法从资料中学到任何东西。 再看一个数学上的二分类例子，其中特征$\mathcal{X}={0,1}^{3}$，$\mathcal{Y}={$O,X$}$，假设有8个hypothesis，都能够在已知数据$D$上对五个样本的分类都达到很好的效果，然而在$D$以外的未知数据上，每个hypothesis分类效果都不同，即$g \approx f$不一定成立。 机器学习的目的，恰恰是在未知数据上的预测效果与真实结果一致，而不是在已知结果上。 以上的例子告诉我们，机器学习想要在$\mathcal D$以外的数据集中得到接近$f$的g是几乎做不到的，我们无法确定某个$g$在经过$D$的学习后，对$f$接近还是不接近，如果我们坚持$f$是不明确的，那么任何$g$都可以说是错误的，。只能保证对$\mathcal D$有很好的分类效果，这个现象被称为no-free-lunch。 NFL定理表明没有一个学习算法可以在任何领域总是产生最好的效果，但至少存在一个$g$能够有较好的性能。 由此，我们需要对机器学习加上一些限制条件。 Probability to the Rescue由上一节可知，机器学习在$D$以外的样本上是做不到完全正确的预测或分类的。因此需要采取一些限制，使其能够对未知的$f$做一些推论。 看下面的一个例子，求orange球所占的比例$\mu$。统计学上的方法是，从所有球中取出N个球作为样本，计算这N个球中orange球的比例$\nu$。那么这个$\nu$就有可能差不多接近$\mu$（probably approximately correct，PAC）。此时可以大致逼近$\mu$。 若取的样本N较小，可能$\nu$会离真实的$\mu$很远，因此需要限定两者的一个误差范围$\epsilon$。根據 Hoeffding’s Inequality，取樣的概率與樣本外的概率關係為： $$\mathbb{P}[|\nu - \mu| &gt; \epsilon] \leq 2\exp(-2\epsilon^{2}N)$$ 此时我们并不需要去知道$\mu$的大小，只需要知道N越大，误差会越小。 Connection to Learning为了将Hoeffding’s Inequality与机器学习对应起来，我们可以先假设我们有一个固定的$h(\mathbf x)$，由$h(\mathbf x)$得到的结果可以对应两种玻璃球的颜色orange，green，其中orange表示结果错误，green表示结果正确。 类似的，通过较大的取样样本N，可以在已知资料集$D$（对应上一节我们随手取出的N个玻璃球）上验证$h(\mathbf x)$的正确率来大致估计$h(\mathbf x)$在资料集$D$之外相对于$f$的正确率。 使用$E_{in}(h)$表示在已知样本集上的正确率，$E_{out}(h)$则表示样本集外的正确率，则可以根据Hoeffding’s Inequality得到： 此时我们不需要知道$E_{out}(h)$，就像上一节我们不需要知道$\mu$（可通过N得到的$\nu$推测）一样，也不需要知道$\mathbb P$，只要N足够大，$E_{in}(h)$跟$E_{out}(h)$自然就会比较小。 上述论证都只是绑定在一个$h$上的，更像是对一个$h$做verification，验证其效果好不好，真正的机器学习是要有选择的，会根据不同的学习资料，返回一个最终能够使用$g$给你。 Connection to Real Learning假设在不同的$h$里，每个$h$都对应一个$E_{in}$跟$E_{out}$： 那就有一个问题：是否在某个集合$\mathcal D$上的例子中$E_{in}$最小的就是最好的$h$呢？ 假设150个人丢同样的硬币，每人丢五次，有大于99%（$1-(1-\frac {1}{2^5})^{150}$）的概率会有人丢出5次都是正面的情况，那么这个情况是一个比较差的采样，因为硬币丢出正面的概率应该为50%。 机器学习中也会遇到类似的情况：在$\mathcal H$中选取经验误差（$E_{in}$）最小的$h$，有可能泛化误差（$E_{out}$）会很大。 对于坏样本与$h(\mathbf x)$，有以下关系图： 只要样本$\mathcal D$对某个$h$属于Bad Data，那么该$D$就是坏训练样本，会让机器学习对$h$的选择受到很大干扰。 对于给定的假设函数$h$，我们定义$E_{in}$很小而$E_{out}$很大的训练集$D$是坏训练样本，那么训练集$\mathcal D$对于全体$h$而言都是坏的几率为： $$\begin{aligned} \mathbb P_{\mathcal {D}}[\text {BAD } \mathcal {D}] &amp;= \mathbb P_{\mathcal {D}}[\text {BAD } \mathcal {D} \text{ for } h_1 \text{ or } \text {BAD } \mathcal {D} \text{ for } h_2 \text{ or … or } \text {BAD } \mathcal {D} \text{ for } h_m]\&amp;\leq \mathbb P_{\mathcal {D}}[\text {BAD } \mathcal {D} \text{ for } h_1] + \mathbb P_{\mathcal {D}}[\text {BAD } \mathcal {D} \text{ for } h_2] + … + \mathbb P_{\mathcal {D}}[\text {BAD } \mathcal {D} \text{ for } h_m]\&amp;\leq 2exp(-2 \epsilon^2N) + 2exp(-2 \epsilon^2N) + … + 2exp(-2 \epsilon^2N)\&amp;\leq 2Mexp(-2 \epsilon^2N)\end{aligned}$$ 这表明$\mathcal H$为有限集M个时，只要训练样本N够多，误差容忍$\epsilon$足够大时，就可以说最终得到的$g$的经验误差和泛化误差是PAC的。]]></content>
      <categories>
        <category>机器学习基石</category>
      </categories>
      <tags>
        <tag>machine learning</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石--Lecture3_Learning with Different Output Space]]></title>
    <url>%2F2019%2F08%2F11%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-lecture3-Learning-with-Different-Output-Space%2F</url>
    <content type="text"><![CDATA[Learning with Different Output Space根据输出空间来分，通常可以划分为分类（classicfication）与回归（regression）两大类，其中分类通常又可以分为二分类与多分类。除此之外，还有结构化学习，这里一类型的输出数据之间存在着结构化的关系（如语音-&gt;句子等）。 对于区分是与否的问题，通常称为binary Classification（二元分类），后面各种复杂的分类，都是基于二分类展开。 0x01_多元分类 区分美国硬币，共有四种2c，5c，10c，25c，该问题是一个四分类，由此可泛化为K分类，二分类是其中特殊的一种。 多分类的常见应用多为“识别”，如图像识别，人脸识别，医疗图像识别等。 0x02_回归，结构化学习有界回归（bounded regression），在统计学中是一个很经典的问题。 $\mathcal{Y}=\mathbb{R}$ or $\mathcal{Y}=[lower,upper] \subset \mathbb{R}$bounded regression——deeply studied in statistics 其他回归类的问题有： 公司数据=》股票价格 气候数据=》预测温度 特点为输入一系列数据，然后输出一个实数。 线性回归已经较为成熟，有许多线程的工具，也可用于构建其他更加复杂的算法。 结构化学习（Structured Learning）：使用机器学习对一个句子里的词性进行分类，表面看是一个多分类问题，但实际上每个单词可能有多个不同的词性，此时进行识别较为困难。但若是将整个句子送进去，从总体来看，就有一定的语法结构可循，属于多分类的一种扩展。 以上两种学习都是从二分类，多分类延伸而来，都是希望机器能学到输出空间中的某种结构。 Learning with Different Data Label yn根据样本来分，可分为监督学习（supervised），无监督学习（unsupervised）和半监督学习（semi-supervised）以及强化学习（Reinforcement Learning），其中强化学习对样本依赖较少，而且是通过奖惩制度来训练。 监督式学习（Supervised），有确定的的期望输出$y$，结合带有label的一组输入，求得目标函数。比如多分类区分1c，5c，10c，25c的硬币。 非监督式学习（Unsupervised），目标较为分散，并不给一个具体的目标，也没有相应的label，让机器自己去找出某种关联。 典型的几个应用： 分群：$\left{\mathbf{x}_{n}\right} \Rightarrow$ cluster $(\mathbf{x})$，可以大致看为“非监督式的多分类问题”，比如自动的将文章分类为多个主题。 密度估计：$\left{\mathbf{x}_{n}\right} \Rightarrow$ density $(\mathbf{x})$,可以大致当做“非监督式的有界回归”，比如通过多个路口的交通报告，确定哪些区域的交通事故较多。 异常值检测：比如网络流量的资料，某几个点比较奇怪（如DOS攻击，或者服务器状态不正常），那么机器会自动的从中找出来。 非监督式学习有很多地方会对应到监督式里的一些概念。 半监督式学习（Semi-supervised）：介于监督式与无监督式学习之间，通常是只对部分样本打上label及确定期望输出，然后让机器进行学习，适用于在打标签很“昂贵”的情况。 强化学习（Reinforcement）：有隐式的目标${y}_{n}$，但不会明确提出来，而是通过奖惩机制告诉机器好坏，让其一步一步学习。比如： 输入（用户，广告选择，用户点击获取收入）=》从而训练广告系统 训练游戏AI 对比几种学习的区别： 其中无监督，半监督及强化学习，都是从监督式学习中演变出来的。 Learning with Different Protocol f ⇒ (xn , yn )训练时数据的输入方式，根据数据是否一次性送入到模型中训练将其分为 batch learning 和 online learning以及active learning。 batch方式，通过喂给机器批量的数据进行学习，如通过输入给机器一批数据，标识出其中的垃圾邮件，让机器自己学会区分。 online方式：会持续不断接收进来的数据，根据资料再不断地更新$g$以提升算法表现。 以垃圾邮件为例，在当前已有算法${g}{t}(\mathbf{x}{t})$的前提下： 接收到一个邮件$\mathbf{x}_{t}$ 使用当前算法${g}{t}(\mathbf{x}{t})$判断是否为垃圾邮件 从用户那里接收期望的输出标签$y_{t}$（如用户认为这是或者不是垃圾邮件），并根据($\mathbf{x}{t},\mathbf{y}{t}$)更新算法${g}_{t}$ 由此可见，online是一个不断更新自己的方式，所以PLA可以很容易的应用到上面。 online learing也不是每接收一个样本就立刻进行训练更新，而是往往等到积累一批数据后在进行训练更新。 强化学习也是持续不断的更新自己，经常也以online方式。 active方式：通过少量label然后机器学习，当机器无法确认分类时，主动询问$f$（代指人），从而不断地提升$g$的表现，属于半监督学习的一种。 相比batch（填鸭式），online（被动接收）方式，active会去主动地获取目标$y_n$进行学习。 Learning with Different Protocol $f ⇒ (x_n ,y_n )$： batch方式是通过所有已知的data进行学习； online方式使用持续不断输入的data； active方式通过有技巧性的获取data； … and more！ Learning with Different Input Space X根据输入的样本的特征来分也可以分为三类：Input features，raw feaures 和 abstract features。 输入特征（Input features，concrete features）：样本$\mathcal{X} \subseteq \mathbb{R}^{d}$的各项维度，都代表了准确，相关的物理意义。 通常输入特征都包含了一些人类智能指定的信息，从ML的家度来看，是属于比较简单的一些东西。 原始特征（Raw Features）：对象自身带有的一些特性，只带有一些简单的物理意义。 以数字识别为例，比如下列图片每个数字都是由16X16的像素点组成的： 如果不做任何处理，只看原始数据，那么此时的raw feature就是这样：16 by 16 gray image：$x≡ (0,0,0.9,0.6,···) \epsilon \mathcal{R}^{256}$。 此时每个样本就是一个256维度的向量。每个维度的物理意义只是代表某个点，而判断数字并不是靠某一个点，而是要结合其他的点，到底有没有点等等的关系，对于ML来说会是一个很复杂的问题。 若加上人类的一些认知，对其进行一些处理，比如数字可能有是否对称，笔画密度如何等。比如1是比较对称的，5是笔画密度较稠密的，根据这点将数字1和5转为为一个二维平面图，y轴代表对称程度，x轴代表密度： 那么此时就是一个二维的向量，对于ML来说会容易很多。 抽象特征（Abstract Features）：从raw feature更加抽象，且每个维度并没有太具体的物理意义，对于ML来说更加困难。 比如用户对音乐进行打分： (userid, itemid, rating) ，使用这个抽象特征，由此预测出哪些用户会对哪些歌曲给出多少评分。（类似的例子还有学生习题系统，广告在线投放等） 对于上述的抽象特征，我们需要帮助每个使用者抽取出真正的feature，比如对歌曲的喜好程度的向量，歌曲曲风的向量，作曲家的向量等等，这些也需要由人或者机器（跟raw feature）一样去做预处理。 个人的理解：其实上述提到的Raw feature，Abstract feature就有点分别对应于推荐系统中的维度表以及事实表一样。 对于机器来说，越抽象的数据（如多维）就困难，往往需要我们对原始数据（raw data）或者抽象数据（abstract data）做一个特征工程（feature engineer），经过预处理后的数据再喂给机器。 特征工程往往能觉得一个机器学习的生死，特征工程可能由人完成（比如手动打标签，过滤，清洗等），也可能由机器自己完成（如深度学习）。]]></content>
      <categories>
        <category>机器学习基石</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习基石--Lecture2_Learning to Answer Yes/No (PLA)]]></title>
    <url>%2F2019%2F08%2F03%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%9F%B3-Learning-to-Answer-Yes-No-PLA%2F</url>
    <content type="text"><![CDATA[问题：银行根据客户的各项资料，决定是否为客户颁发信用卡？ 0x01_感知机感知机是神经网络的基础，与线性回归（Linerar Regression），逻辑回归（Logistics Regression）等模型也非常类似，是一种典型的线性模型。 原始的感知机主要用于解决二分类问题，比如前面提到的是否颁发信用卡就是一个二分类问题，模拟如下图： 客户的特征可以视为多个向量组合，使用一个权重$w$去跟客户的各项向量$x$做一个內积，再减去设定的一个门槛，从而决定是否为客户颁发信用卡。假设存在一个完美的函数$f$,该函数是未知的，我们只能够从训练集D中使用算法A，从假设集$\mathcal{H}$中求得一个无限逼近$f$的函数$g$。 为了简化公式，threshold也当做一个维度0，合并到客户的维度里，从而得到一个高维a$$\begin{aligned} h(\mathbf{x}) &amp;=\operatorname{sign}\left(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)-\text { threshold }\right) \&amp;=\operatorname{sign}\left(\left(\sum_{i=1}^{d} w_{i} x_{i}\right)+\underbrace{(-\text { threshold })}{w{0}} \cdot \underbrace{(\text { +1})}{w{0}} \right) \ &amp;=\operatorname{sign}\left(\sum_{i=0}^{d} w_{i} x_{i}\right) \ &amp;=\operatorname{sign}\left(\mathbf{w}^{T} \mathbf{x}\right) \end{aligned}$$ 0x02_PLA 先忽略$w_{0}$，那么$h(\mathbf{x})$就有两个维度，得到以上二维图其中$\mathbf{X}$代表图上的点，y则代表yes/no，$h$则是这个二维图上的一个个点组成，由此得到的$h$是一个线性分类器（perceptron）。 以上二维图为例，我们要找的$g$有无限多条直线，我们并不可能去对无限多条线一条一条的找，因此通常采用这种方法： 先确定一条线，得到参数$w$。 每次都根据错误的点进行修正，得到一条新的线，更新$w$。 重复上述步骤直到所有的样本点都能够被$w$正确预测。 这种通常称为感知器学习法（PLA,Perceptron Learning Algorithm） 0x03_Fun Time 这道题想表达的意思就是演算法每次都会尝试去修正得更好。 0x04_收敛性PLA是否会停下来？答：取决于每次回归后是否仍有错误的点，以及是否线性可分（Linear Separability）。 假设为线性可分，且有完美的$w$，则$w$跟向量$x$，y（任意点）相乘，总存在一个最小值$\min {n} y{n} \mathbf{w}{f}^{T} \mathbf{x}{n}$大于0： 由上可知，每次$w$进行更新，$w_{f}^{T} w_{t+1}$的值就会越来越大（即两者越来越接近），虽然內积越大，代表向量越接近，但是由于还受到向量长度（单位向量）的影响，因此还得对向量做一个标准化，但已经能够说明收敛的趋势了。 对于非完美的$w$，mistake的点的內积会小于等于0；再结合新一轮的$w$由上一轮的$w$加上mistake的点求得，由这两点可做出以下推导： 由此可知，每次$w$成长的速度，最多也就到距离最远的那个点。 接下来对向量进行标准化，假设$w_0$为0，在经过T轮后可得到：$$\frac{w_{f}}{\left|w_{f}\right|} \frac{w_{T}}{\left|w_{T}\right|} \geq \sqrt{T} * \text { constant }$$推导过程： 经过T轮纠错，由$\mathbf{w}{f}^{T} \mathbf{w}{t+1}\geq \mathbf{w}{f}^{T} \mathbf{w}{t}+\min {n} y{n} \mathbf{w}{t}^{T} \mathbf{x}{n}$推导出：$$w_{f}^{T} w_{T} \geq w_{f}^{T} w_{T-1}+\min {n} y{n} w_{f}^{T} x_{n} \geq T \min {n} y{n} w_{f}^{T} x_{n}$$ 再由$\left|\mathbf{w}{t+1}\right|^{2}\leq\left|\mathbf{w}{t}\right|^{2}+\max {n}\left|y{n} \mathbf{x}{n}\right|^{2}$可推导出：$$\left|w{T}\right|^{2} \leq\left|w_{T-1}\right|^{2}+\max {n}\left|x{n}\right|^{2} \leq T \max {n}\left|x{n}\right|^{2}\left|w_{T}\right| \leq \sqrt{T} \max {n}\left|x{n}\right|$$综合上面的式子得到：$$\frac{w_{f}^{T}}{\left|w_{f}^{T}\right|} \frac{w_{T}}{\left|w_{T}\right|} \geq \frac{T \min {n} y{n}^{T} w_{f}^{T} x_{n}}{\left|w_{f}^{T}\right| \sqrt{T} \max {n}\left|x{n}\right|}=\sqrt{T} \frac{\min {n} y{n} \frac{w_{f}^{T}}{ \left| w_{f}^{T} \right|} x_{n}}{\max {n}\left|x{n}\right|}=\sqrt{T} * \text { constant }$$ 又因为两条标准化向量最大值为1： $$\begin{array}{c}{1 \geq \frac{w_{f}^{T}}{\left|w_{f}^{T}\right|} \frac{w_{T}}{\left|w_{T}\right|} \geq \sqrt{T} * \text { constant }} \ {\frac{1}{\text {constant}^{2}} \geq T}\end{array}$$ 假设令$\max {n}|x|=R, \rho=\min _{n} y{n} \frac{w_{f}^{T}}{\left|w_{f}^{T}\right|} x_{n}$，其中constant为$\frac{\min {n} y{n} \frac{w_{f}^{T}}{ \left| w_{f}^{T} \right|} x_{n}}{\max {n}\left|x{n}\right|}$，则有： $$T \leq \frac{R^{2}}{\rho^{2}}$$ 由此可说明PLA会在有限步内收敛。 0x05_优缺点及改进PLA优缺点都很明显，优点是简单，易于实现，缺点是，由于开头是设置线性可分的，然而实现并无法知道是否线性可分，即使有$T \leq \frac{R^{2}}{\rho^{2}}$，但$\rho$也是由假设$w_f$得到的，因此假如将PLA用在线性不可分的数据中时，会导致PLA永远都在循环。 为了避免上述问题，将PLA的条件放宽点，不再要求所有的样本都能正确的分开，而是犯错的点尽可能地少，于是问题就转化为了： $$\arg \min {w} \sum{n=1}^{N} 1\left{y_{n} \neq \operatorname{sign}\left(w^{T} x_{n}\right)\right}$$ 这个是经典的NP-hard问题，无法求得其最优解，因此只能尽可能的接近其最优解。 根据上述四项，PLA的变种Pocket Algorithm，每次都保留当前最好的$w$，当进行修正后得到新的$w_{new}$，将其与原来的$w$进行总体效果上的对比，保留效果最好的那一个，重复迭代足够多的次数后返回$w_{best}$。 参考：机器学习基石–PLA]]></content>
      <categories>
        <category>机器学习基石</category>
      </categories>
      <tags>
        <tag>IT</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Scala: try-with-resources]]></title>
    <url>%2F2019%2F07%2F25%2FScala-try-with-resources%2F</url>
    <content type="text"><![CDATA[本文主要针对Scala: try-with-resources进行一些记录，加上个人的理解。文章介绍了scala中对异常的捕获常见的几个误区，主要有以下几点： 0x00_Mistake 1: swallow exception in finally123456val file: InputStream = _try &#123; // read from stream&#125; finally &#123; file.close()&#125; 以上方式有个问题，假如try中抛出了一个异常，此时finally块中的close也抛出了一个异常，那么try中的异常将会被finally块中的异常覆盖掉，验证如下： 12345678try&#123; throw new IOException()&#125; finally &#123; throw new FileNotFoundException()&#125;// 输出：java.io.FileNotFoundException ... 36 elided 为了避免try块的异常被覆盖掉，可以使用addSuppressed方法，将try块中的异常添加到finally块中的异常中（本质使用了一个Array保存），避免丢失： 12345678910111213var e:Exception = nulltry&#123; e = new IOException()&#125; finally &#123; val e2 = new FileNotFoundException() e2.addSuppressed(e) // jdk1.7实现 throw e2&#125;// 输出：java.io.FileNotFoundException ... 38 elided Suppressed: java.io.IOException ... 37 more 0x01_Mistake 2: Catching Exception/Throwable1234567def cleanly[A, B](resource: A)(cleanup: A =&gt; Unit)(doWork: A =&gt; B): Try[B] = &#123; try &#123; Success(doWork(resource)) &#125; catch &#123; case e: Exception =&gt; Failure(e) &#125; finally ... 以上实现有以下几个问题： Throwable的子类不会被捕获 有一类特殊的异常不应该被捕获：InterruptedException， 可以查看这里，因为InterruptedException要从具体场景出发。若只是表示线程中断，此时InterruptedException是个中断标志，那么throw即可；若是A方法计算中调用B方法，而B方法抛出了InterruptedException，此时A方法跟着抛出的InterruptedException代表方法所要完成的计算失败了，要取消该操作，则应该使用Thread.currentThread().interrupt()设置中断标志，并进行相关业务处理。 所有的Exception都会被捕获，即使是一些系统性的异常（如OOM）。 scala中可以使用以下方式： 1case NonFatal(e) =&gt; Failure(e) NonFatal不会捕获如InterruptedException 或者OOM等这类特殊的异常。 0x02_Mistake 3: Not closing resource在使用上面提到的NonFatal时，也需要考虑到一些场景，会导致打开的resource没有正确的关闭，如下代码： 1234567891011121314151617def apply[C &lt;: Closeable, R](resource: =&gt; C)(f: C =&gt; R): Try[R] = Try(resource).flatMap(resourceInstance =&gt; &#123; try &#123; val returnValue = f(resourceInstance) Try(resourceInstance.close()).map(_ =&gt; returnValue) &#125; catch &#123; case NonFatal(exceptionInFunction) =&gt; try &#123; resourceInstance.close() Failure(exceptionInFunction) &#125; catch &#123; case NonFatal(exceptionInClose) =&gt; exceptionInFunction.addSuppressed(exceptionInClose) Failure(exceptionInFunction) &#125; &#125; &#125;) 以上代码捕获了NonFatal，但此时若抛出的为fatal exception（如InterruptedException），那么此时打开的resource也会无法正常关闭，因此有一点原则要牢牢遵守： The general rule is you should always try to close (release) the resource. Even in case of “fatal” exception. 0x03_Mistake 4: Swallowing exceptions from closefinally块中close resource的时候可能也会抛出异常，异常中可能带有一些我们需要的信息，此时需要考虑到使用IOUtils.closeQuietly是否合适。通常来说close抛出的异常可能代表以下两种问题： 内部的buffer没有成功flush到磁盘。 在关闭文件描述符的时候发生了一些内部的错误。 对于第二点，我们没法做任何处理，但第一点的话，假如我们是在写文件（socket，etc），此时exception不应该被忽略，我们没法确保操作成功与否。因此，我们需要告诉调用者操作失败，而不是忽略exception。当然，如果只有读操作，那么该exception忽略也无所谓。 0x04_try-with-resources in Scala由以上几点可以确定一个try-with-resources写法，并将其封装为工具 12345678910111213141516171819202122232425262728def withResources[T &lt;: AutoCloseable, V](r: T)(f: T =&gt; V): V = &#123; val resource: T = r require(resource != null, "resource is null") var exception: Throwable = null try &#123; f(resource) &#125; catch &#123; case NonFatal(e) =&gt; exception = e throw e &#125; finally &#123; closeAndAddSuppressed(exception, resource) &#125;&#125;private def closeAndAddSuppressed(e: Throwable, resource: AutoCloseable): Unit = &#123; if (e != null) &#123; try &#123; resource.close() &#125; catch &#123; case NonFatal(suppressed) =&gt; e.addSuppressed(suppressed) &#125; &#125; else &#123; resource.close() &#125;&#125; 看评论的时候，此处实现有个小问题，就是在catch中，假设try抛出的为Fatal异常，那么此处将导致e为null，closeAndAddSuppressed中直接close resource，此时Fatal（实际上此处InterruptedException跟Fatal混在了一起）将会被忽略。 总结 不要捕获直接Exception或者Throwable。 finally块中要多个心眼，不要让finally中的exception覆盖了try块中的exception。 如果只有read操作，那么finally的异常覆盖了也无关紧要。]]></content>
      <categories>
        <category>读书笔记</category>
      </categories>
      <tags>
        <tag>IT</tag>
        <tag>Scala</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[docker+hexo+git搭建个人Blog]]></title>
    <url>%2F2019%2F06%2F26%2Fdocker-hexo-git%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BABlog%2F</url>
    <content type="text"><![CDATA[前置准备： 一个git账号 docker基础知识 一台Linux云服务器（其实本地windows也行，只不过较为麻烦，第二节有提到windows如何搭建docker的文章） 0x01_搭建hexo初始模板 首先创建一个hexo项目。12345678910# 以epel方式安装nodejsyum install -y epel-release &amp;&amp; yum install -y nodejs# 安装hexonpm install hexo-cli -g# hexo初始化模板，目录为bloghexo init blogcd blognpm install# 默认4000端口hexo server 如上，访问 http://ip:4000 查看效果。 将hexo项目上传至自己的github。 0x02_搭建docker环境（Linux环境） 宿主机安装docker 123# linux安装dockersudo yum update -ysudo yum install -y docker 编辑/etc/docker/daemon.json文件，加入以下内容 123&#123; "hosts": [ "unix:///var/run/docker.sock","tcp://0.0.0.0:2376"]&#125; 添加DOCKER_HOST 1sudo echo &quot;export DOCKER_HOST=tcp://0.0.0.0:2376&quot; &gt;&gt; ~/.bashrc &amp;&amp; source ~/.bashrc 将当前用户添加进docker用户组，之后需要登出session再重新登录 12# Allow your user to access the Docker CLI without needing root access.sudo usermod -aG docker $USER 启动docker 12sudo systemctl enable dockersudo systemctl start docker 验证docker是否安装成功，如界面出现hello-world则代表安装成功。 1sudo docker run hello-world 关于windows环境如何搭建docker，可以参考此篇文章末尾。 0x03_基于Dockerfile构建可持续发布的blog镜像新建一个目录，建立文件名为Dockerfile，构建image。其中git config --global url.&quot;https://{token}:@github.com/&quot;.insteadOf &quot;https://github.com/&quot;中的{token}在github中配置。 GitHub access token：Your Profile → Settings → Personal Access Tokens → Generate New Token，勾选repo并生成即可获得对应token，如图： 123456789101112131415161718192021222324252627282930FROM centosLABEL author=zxk email=usename@gmail.com# 更新yumRUN yum clean all &amp;&amp; yum update -y &amp;&amp; \# 安装vim并设置中文支持yum install -y vim &amp;&amp; echo -e "set fileencodings=utf-8,ucs-bom,gb18030,gbk,gb2312,cp936\nset termencoding=utf-8\nset encoding=utf-8" &gt; ~/.vimrc &amp;&amp; \# 通过epel方式安装nodejsyum install -y epel-release &amp;&amp; yum install -y nodejs &amp;&amp; \# 安装hexo及hexo部署git插件npm install -g hexo-cli &amp;&amp; npm install hexo-deployer-git --save &amp;&amp; \# 安装gityum install -y git-core &amp;&amp; \# 通过token方式设置git免密git config --global url."https://&#123;token&#125;:@github.com/".insteadOf "https://github.com/" &amp;&amp; \# 设置用户信息git config --global user.name "username" &amp;&amp; \git config --global user.email "username@gmail.com" &amp;&amp; \# clone你的hexo项目git clone https://github.com/username/repo /blog &amp;&amp; \# 初始化blognpm install /blogEXPOSE 4000WORKDIR /blog# 每次run都自动从git更新，启动hexo serverCMD git pull &amp;&amp; hexo clean &amp;&amp; hexo g &amp;&amp; hexo s 在目录下执行docker build --no-cache -t imageName .即可构建image。 0x04_预览及发布基于以上步骤构建的image，可通过下列命令启动一个container 1docker run -itd -p 4000:4000 imageName 在启动后即可通过访问 http://ip:4000 查看效果，至于发布到gitpage的话，可以参考官方教程，这里简单描述下。 在你的github上创建一个仓库，名为username.github.io,切记username要替换为你的github名字。 连接上你的container 。 1docker exec -it containerId bash 在hexo项目所在目录下编辑 _config.yml，添加git关联 123456# Deployment## Docs: https://hexo.io/docs/deployment.htmldeploy: type: git repo: https://github.com/username/username.github.io.git branch: master 接着执行下列命令，执行清理，生成文件，发布，即可自动部署到gitpage。访问 https://username.github.io/ 即可查看效果。 123hexo cleanhexo generatehexo doploy 0x05 结尾鉴于本人对前端一窍不通，花了好几天才做出这个blog，主题是套用blinkfox大佬的hexo-theme-matery，然后加上自己的一些小修改，在此表示非常感谢所有为开源做出一份贡献的人们。]]></content>
      <categories>
        <category>IT技术</category>
      </categories>
      <tags>
        <tag>IT</tag>
        <tag>docker</tag>
        <tag>hexo</tag>
        <tag>blog</tag>
      </tags>
  </entry>
</search>
